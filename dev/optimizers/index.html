<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimizers · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../resources/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li class="is-active"><a class="tocitem" href>Optimizers</a><ul class="internal"><li><a class="tocitem" href="#Generating-Training-Data"><span>Generating Training Data</span></a></li><li><a class="tocitem" href="#Inverse-Modeling-Implementation"><span>Inverse Modeling Implementation</span></a></li><li><a class="tocitem" href="#Choosing-Different-Optimizers"><span>Choosing Different Optimizers</span></a></li><li><a class="tocitem" href="#Extending-Optimization-Libraries"><span>Extending Optimization Libraries</span></a></li><li><a class="tocitem" href="#Appendix:-Benchmark-Results-for-Different-Optimizers"><span>Appendix: Benchmark Results for Different Optimizers</span></a></li></ul></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../smt/">Managing Numerical Experiments with SMT</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Resources</a></li><li class="is-active"><a href>Optimizers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimizers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/optimizers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Optimizers"><a class="docs-heading-anchor" href="#Optimizers">Optimizers</a><a id="Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Optimizers" title="Permalink"></a></h1><p>ADCME provides a rich class of optimizers and acceleration techniques for conducting inverse modeling. The programming model also allows for easily extending ADCME with customer optimizers. In this section, we show how to take advantage of the built-in optimization library by showing how to solve an inverse problem–estimating the diffusivity coefficient of a Poisson equation from sparse observations–using different kinds of optimization techniques.    </p><h2 id="Generating-Training-Data"><a class="docs-heading-anchor" href="#Generating-Training-Data">Generating Training Data</a><a id="Generating-Training-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Generating-Training-Data" title="Permalink"></a></h2><p>Consider the Poisson equation in 2D:</p><div>\[\begin{aligned}
\nabla  \cdot (\kappa (x,y)\nabla u(x,y)) &amp;= f(x) &amp; (x,y) \in \Omega\\ 
u(x,y) &amp;=0 &amp; (x,y)\in \partial \Omega
\end{aligned}\tag{1}\]</div><p>Here <span>$\Omega$</span> is a L-shaped domain, which can be loaded using <code>meshread</code>. </p><div>\[f(x,y) = \sin\left(2\pi y+\frac{\pi}{8}\right)\]</div><p>and the diffusivity coefficient <span>$\kappa(x,y)$</span> is given by </p><div>\[\kappa(x,y) = 2+e^x - y^2\]</div><p>We can solve Equation 1 using a standard finite element method. Here, we use NNFEM.jl solve the PDE.</p><p>First, we specify the types of element in the domain</p><pre><code class="language-julia">using Revise
using ADCME
using NNFEM
using LinearAlgebra
using PyPlot
using JLD2 

nodes, elems = meshread(&quot;$(splitdir(pathof(NNFEM))[1])/../deps/Data/lshape_dense.msh&quot;)
elements = []
prop = Dict(&quot;name&quot;=&gt; &quot;Scalar1D&quot;, &quot;kappa&quot;=&gt;2.0)

for k = 1:size(elems,1)
    elnodes = elems[k,:]
    ngp = 2
    coord = nodes[elnodes,:]
    push!(elements, SmallStrainContinuum(coord, elnodes, prop, ngp))
end</code></pre><p>Next, we impose proper boundary conditions on the domain</p><pre><code class="language-julia">EBC = zeros(Int64, size(nodes,1))
FBC = zeros(Int64, size(nodes,1))
g = zeros(size(nodes,1))
f = zeros(size(nodes,1))

bd = find_boundary(nodes, elems)
EBC[bd] .= -1

ndims = 1
domain = Domain(nodes, elements, ndims, EBC, g, FBC, f)</code></pre><p>In order to use the custom operators such as <code>compute_body_force_terms1</code>, we need to initialize the domain so that the domain domain is copied to the C++ dynamic library memory. </p><pre><code class="language-julia">init_nnfem(domain)</code></pre><p>We can now assemble the coefficient matrix and assemble the right hand side:</p><pre><code class="language-julia">α = 0.4*π/2
d = [cos(α);sin(α)]
f = (x,y)-&gt;sin(2π*y + π/8)
fext = compute_body_force_terms1(domain, f)

sol = zeros(domain.nnodes)
gnodes = getGaussPoints(domain)
x, y = gnodes[:,1], gnodes[:,2]

kref = @. 2.0 + exp(x) - y^2
k = vector(1:4:4getNGauss(domain), kref, 4getNGauss(domain)) + vector(4:4:4getNGauss(domain), kref, 4getNGauss(domain))
k = reshape(k, (getNGauss(domain),2,2))
K = s_compute_stiffness_matrix1(k, domain)</code></pre><p>Note here we have use <code>vector</code> to formulate the diffusivity tensor, which is a <span>$2\times 2$</span> tensor for each Gauss point. Here, each component of <code>k</code> has the form </p><div>\[\begin{bmatrix}
   {\kappa ({x_i},{y_i})} &amp; {}   \\
   {} &amp; {\kappa ({x_i},{y_i})}   
\end{bmatrix}\]</div><p>where <span>$(x_i,y_i)$</span> is the corresponding Gauss points. </p><p>The linear system can now be solved and the solution is stored in <code>sol</code> with </p><pre><code class="language-julia">S = K\fext
sess = Session(); init(sess)
sol[domain.dof_to_eq] = run(sess, S)</code></pre><p>Finally, let us visualize <span>$\kappa(x,y)$</span> and solution <span>$u(x,y)$</span></p><pre><code class="language-julia">figure(figsize=(10, 5))
subplot(121)
title(&quot;\$\\kappa(x,y)\$&quot;)
visualize_scalar_on_undeformed_body(kref, domain)
subplot(122)
visualize_scalar_on_undeformed_body(sol, domain)
title(&quot;\$u(x,y)\$&quot;)
using Random; Random.seed!(233)
idx = rand(findall(domain.dof_to_eq), 900)
scatter(domain.nodes[idx,1], domain.nodes[idx,2], color=&quot;magenta&quot;, s=5)
savefig(&quot;Data/reference.png&quot;)

@save &quot;poisson.jld2&quot; domain kref sol </code></pre><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/reference.png?raw=true" alt/></p><p>The magenta dots in the right panel shows the observed state variables. </p><h2 id="Inverse-Modeling-Implementation"><a class="docs-heading-anchor" href="#Inverse-Modeling-Implementation">Inverse Modeling Implementation</a><a id="Inverse-Modeling-Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Inverse-Modeling-Implementation" title="Permalink"></a></h2><p>The above forward computation is implemented in an AD-capable way; that is, all the operators in the above forward computation possess automatic differentiation capabilities. Therefore, we can easily cast the above code to an inverse modeling version. The inverse problem we want to solve is</p><blockquote><p>Suppose we have observed some state variables (the magenta dots in the right panel above), and suppose the source term and boundary conditions are known, we want to estimate a <strong>spatially-varying</strong> diffusivity coefficient <span>$\kappa(x,y)$</span>.</p></blockquote><p>Note that in terms of degrees of freedom, the number of unknowns (<span>$\kappa(x_i,y_i)$</span> at each Gauss points) is far more than the number of observations. Therefore the inverse problem is underdetermined, making it necessary to adopt regularization. Here the neural network representation of <span>$\kappa(x,y)$</span> is a form of regularization.</p><p>Here we show the content for <code>compute_loss.jl</code>, which is used in a later text. We use <code>ArgParse</code> to manage the command line parameters </p><pre><code class="language-julia">using ArgParse
using DelimitedFiles
using PyPlot

function parse_commandline()
    s = ArgParseSettings()

    @add_arg_table! s begin
        &quot;--linesearch&quot;
        &quot;--optimizer&quot;
        &quot;--alpha&quot;
            arg_type = Float64
            default = 1.0
        &quot;--atype&quot;
            arg_type = Int64 
            default = 1
        &quot;arg1&quot;
    end

    return parse_args(s)
end

parameters = parse_commandline()
println(parameters)


reset_default_graph() 
@load &quot;poisson.jld2&quot; domain kref sol 
init_nnfem(domain)
α = 0.4*π/2
d = [cos(α);sin(α)]
f = (x,y)-&gt;sin(2π*y + π/8)
fext = compute_body_force_terms1(domain, f)

gnodes = getGaussPoints(domain)
x, y = gnodes[:,1], gnodes[:,2]


k0 = fc([x y], [20,20,20,1])|&gt;squeeze
k0 = softplus(k0 + 3.0) + 1e-6 # to avoid factorization error 

k = vector(1:4:4getNGauss(domain), k0, 4getNGauss(domain)) +vector(4:4:4getNGauss(domain), k0, 4getNGauss(domain))
k = reshape(k, (getNGauss(domain),2,2))
K = s_compute_stiffness_matrix1(k, domain)
S = K\fext
s = vector(findall(domain.dof_to_eq), S, domain.nnodes)
using Random; Random.seed!(233)
idx = rand(findall(domain.dof_to_eq), 900)
loss = mean((s[idx] - sol[idx])^2) * 1e10</code></pre><p>One special thing about this script is that we formulate the loss function <code>loss</code> using the hypothetical observation <code>s[idx]</code> and true observation <code>sol[idx]</code>. Another noticeable difference is that we assume that we do not know <span>$\kappa(x,y)$</span> and therefore we approximate <span>$\kappa(x,y)$</span> using a neural network</p><pre><code class="language-julia">k0 = fc([x y], [20,20,20,1])|&gt;squeeze
k0 = softplus(k0 + 3.0) + 1e-6 # to avoid factorization error </code></pre><p>We transform the neural network using <code>softplus</code> and add a small value to avoid negative or close to zero <span>$\kappa(x,y)$</span>, which is clearly not physical. </p><h2 id="Choosing-Different-Optimizers"><a class="docs-heading-anchor" href="#Choosing-Different-Optimizers">Choosing Different Optimizers</a><a id="Choosing-Different-Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Choosing-Different-Optimizers" title="Permalink"></a></h2><h3 id="(Accelerated)-Gradient-Descent-Optimizers"><a class="docs-heading-anchor" href="#(Accelerated)-Gradient-Descent-Optimizers">(Accelerated) Gradient Descent Optimizers</a><a id="(Accelerated)-Gradient-Descent-Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#(Accelerated)-Gradient-Descent-Optimizers" title="Permalink"></a></h3><p>A straightforward way to estimate the weights and biases of the neural network is using gradient descent (GD) methods. Variations of gradient descent methods exist, and a large class of such variations is GD with momentum. In ADCME, the following optimizers are provided as built-in gradient descent optimizers</p><pre><code class="language-none">Descent Momentum Nesterov 
RMSProp ADAM RADAM 
AdaMax ADAGrad ADADelta 
AMSGrad NADAM</code></pre><p>Additionally, line search algorithms are very important to accelerate and safeguard convergence. The line search algorithms can be accessed from  <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">LineSearches.jl</a>. Examples include</p><pre><code class="language-none">HagerZhang
BackTracking
MoreThuente
Static</code></pre><p>The following script shows how to use those optimizers and line search algorithms:</p><pre><code class="language-julia">using LineSearches
using Revise
using ADCME
using NNFEM
using LinearAlgebra
using PyPlot
using JLD2 
using Statistics


include(&quot;compute_loss.jl&quot;)
ls = BackTracking()

amsgrad = AMSGrad()
sess = Session(); init(sess)
uo = UnconstrainedOptimizer(sess, loss)
x0 = getInit(uo)

for i = 1:1000
    global x0 
    f, df = getLossAndGrad(uo, x0)
    g = apply!(amsgrad, x0, df)
    α, fx = linesearch(uo, f, df, ls, 100.0)
    x0 -= α*Δ 
end
update!(uo, x0)</code></pre><p>The above code also show a typical routine for iterative optimization algorithms</p><pre><code class="language-none">getLossAndGrad → getSearchDirection → setSearchDirection → linesearch</code></pre><p>Don&#39;t forget <code>update!</code> after the optimization is finished. Also don&#39;t forget <code>reset_default_graph()</code> (in <code>compute_loss.jl</code>) before creating any graph nodes. </p><p>The following figures show the loss function and estimated <span>$\kappa(x,y)$</span> and <span>$u(x,y)$</span>.</p><center><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/opt_loss.png?raw=true" width="50%"></center><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/kandsol.png?raw=true" alt/></p><h3 id="Quasi-Newton-Optimizers"><a class="docs-heading-anchor" href="#Quasi-Newton-Optimizers">Quasi-Newton Optimizers</a><a id="Quasi-Newton-Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Quasi-Newton-Optimizers" title="Permalink"></a></h3><p>We can also use quasi-Newton optimizers, which in general converge much faster but are more computationally expensive and consume more memory. ADCME provides <a href="@ref"><code>LBFGS</code></a> optimizer, with preconditioners as optional arguments.  </p><p>To use <code>LBFGS</code>, we can simply substitute the gradient descent optimizers with <code>LBFGS</code></p><pre><code class="language-julia">lbfgs = Optimizer.LBFGS()
...
for i = 1:100
    ...
    Δ = getSearchDirection(lbfgs, x0, df)
    ...
end
update!(uo, x0)</code></pre><h3 id="Anderson-Acceleration"><a class="docs-heading-anchor" href="#Anderson-Acceleration">Anderson Acceleration</a><a id="Anderson-Acceleration-1"></a><a class="docs-heading-anchor-permalink" href="#Anderson-Acceleration" title="Permalink"></a></h3><p>Anderson acceleration has been used widely in nature science such as chemistry for solving the fixed point problem </p><div>\[f(x) = x\]</div><p>It is computationally cheap, making it attractive for accelerating the gradient descent methods and replacing expensive line searches. The idea is that the gradient descent (<span>$g^k$</span> is the search direction, which may not coincide with the negative gradient direction) </p><div>\[x^{k+1} = x^k + \alpha^k g^k\]</div><p>can be viewed as solving a fixed point system </p><div>\[x = x + \alpha^k g^k(x)\]</div><p>In ADCME, Anderson acceleration is implemented in  <a href="@ref"><code>AndersonAcceleration</code></a> <sup class="footnote-reference"><a id="citeref-cvx" href="#footnote-cvx">[cvx]</a></sup>. The following script shows how to use <code>AndersonAcceleration</code> with a gradient descent optimizer</p><pre><code class="language-julia">using LineSearches
using Revise
using ADCME
using NNFEM
using LinearAlgebra
using PyPlot
using JLD2 
using Statistics
include(&quot;compute_loss.jl&quot;)

adam = Descent()
aa = AndersonAcceleration()
sess = Session(); init(sess)
uo = UnconstrainedOptimizer(sess, loss)
x0 = getInit(uo)
for i = 1:1000
    global x0 
    f, df = getLossAndGrad(uo, x0)
    Δ = getSearchDirection(adam, x0, df)
    x0 = apply!(aa, x0, Δ)
end
update!(uo, x0)</code></pre><p>Here we show the loss profile for different acceleration strategies on <code>AdaMax</code> optimizer:</p><table><tr><th style="text-align: right">Anderson Acceleration Type I</th><th style="text-align: right">Anderson Acceleration Type II</th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/aaloss.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/aa2loss.png?raw=true" alt/></td></tr><tr><td style="text-align: right">Static</td><td style="text-align: right">BackTracking</td></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/staticloss.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/btloss.png?raw=true" alt/></td></tr></table><h2 id="Extending-Optimization-Libraries"><a class="docs-heading-anchor" href="#Extending-Optimization-Libraries">Extending Optimization Libraries</a><a id="Extending-Optimization-Libraries-1"></a><a class="docs-heading-anchor-permalink" href="#Extending-Optimization-Libraries" title="Permalink"></a></h2><p>To extend optimization library, users only need to define an optimization method structure and a function <code>apply!</code> with a signature </p><pre><code class="language-julia">apply!(opt, x, Δ) -&gt; d</code></pre><p>where <code>opt</code> is the optimizer, <code>x</code> is the current state, <code>Δ</code> is the gradient direction, and <code>d</code> is the search direction. Looking at the example, such as <code>ADAM</code>, in the source code is helpful.</p><h2 id="Appendix:-Benchmark-Results-for-Different-Optimizers"><a class="docs-heading-anchor" href="#Appendix:-Benchmark-Results-for-Different-Optimizers">Appendix: Benchmark Results for Different Optimizers</a><a id="Appendix:-Benchmark-Results-for-Different-Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Appendix:-Benchmark-Results-for-Different-Optimizers" title="Permalink"></a></h2><p>Here we present a benchmark result for different optimizers for solving Equation 1. We consider two choices for <span>$f(x,y)$</span>:</p><ul><li><p>High frequency: <span>$f(x,y) = \sin\left(4\pi y+\frac{\pi}{8}\right)$</span></p></li><li><p>Low frequency: <span>$f(x,y) = \sin\left(2\pi y+\frac{\pi}{8}\right)$</span></p></li></ul><p>Additionally, in the inverse modeling, we consider two cases: very sparse data (<span>$n_{\text{obs}}=20$</span>) and moderate amount of data (<span>$n_{\text{obs}}=900$</span>). Here <span>$n_{\text{obs}}$</span> is the number of observations. In the latter case, the amount of data consists roughly 70%~80% of the degrees of freedom. </p><p>The following shows reference solutions and observation distributions for <span>$u(x,y)$</span>. </p><table><tr><th style="text-align: right"><span>$n_{\text{obs}}$</span></th><th style="text-align: right">Low Frequency</th><th style="text-align: right">High Frequency</th></tr><tr><td style="text-align: right">20</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/reference20-1.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/reference20-2.png?raw=true" alt/></td></tr><tr><td style="text-align: right">900</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/reference900-1.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/reference900-2.png?raw=true" alt/></td></tr></table><p>In the following plot, we show the normalized loss function versus the number of function evaluations/gradient evaluations. The normalized loss function is defined as</p><div>\[{{{1 \over N}\sum\limits_{i = 1}^N {{{({u_{{\rm{obs}}}}({x_i}) - {u_\theta }({x_i}))}^2}} } \over {{1 \over N}\sum\limits_{i = 1}^N {{u_{{\rm{obs}}}}{{({x_i})}^2}} }}\]</div><p>Here <span>${{u_{{\rm{obs}}}}({x_i})}$</span> is the observation function value at <span>$x_i$</span> and <span>${{u_\theta }({x_i})}$</span> is the hypothetical solution computed using the neural network (<span>$\theta$</span> denotes the weights and biases of the neural network).</p><p>Some of the optimizers break (encountering <code>Inf</code> or <code>NaN</code>) during the optimization process. We do not include them in the plot.  </p><h3 id="Low-Frequency,-n_{\\text{obs}}20"><a class="docs-heading-anchor" href="#Low-Frequency,-n_{\\text{obs}}20">Low Frequency, <span>$n_{\text{obs}}=20$</span></a><a id="Low-Frequency,-n_{\\text{obs}}20-1"></a><a class="docs-heading-anchor-permalink" href="#Low-Frequency,-n_{\\text{obs}}20" title="Permalink"></a></h3><p><strong>Function Evaluations</strong></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/fn-20-1.png?raw=true" alt/></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/aa-fn-20-1.png?raw=true" alt/> <strong>Gradient Evaluations</strong></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/g-20-1.png?raw=true" alt/></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/aa-fn-20-1.png?raw=true" alt/></p><h3 id="Low-Frequency,-n_{\\text{obs}}900"><a class="docs-heading-anchor" href="#Low-Frequency,-n_{\\text{obs}}900">Low Frequency, <span>$n_{\text{obs}}=900$</span></a><a id="Low-Frequency,-n_{\\text{obs}}900-1"></a><a class="docs-heading-anchor-permalink" href="#Low-Frequency,-n_{\\text{obs}}900" title="Permalink"></a></h3><p><strong>Function Evaluations</strong></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/fn-900-1.png?raw=true" alt/></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/aa-fn-900-1.png?raw=true" alt/></p><p><strong>Gradient Evaluations</strong></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/g-900-1.png?raw=true" alt/></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/aa-g-900-1.png?raw=true" alt/></p><h3 id="High-Frequency,-n_{\\text{obs}}20"><a class="docs-heading-anchor" href="#High-Frequency,-n_{\\text{obs}}20">High Frequency, <span>$n_{\text{obs}}=20$</span></a><a id="High-Frequency,-n_{\\text{obs}}20-1"></a><a class="docs-heading-anchor-permalink" href="#High-Frequency,-n_{\\text{obs}}20" title="Permalink"></a></h3><p><strong>Function Evaluations</strong></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/fn-20-2.png?raw=true" alt/></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/aa-fn-20-2.png?raw=true" alt/> <strong>Gradient Evaluations</strong></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/g-20-2.png?raw=true" alt/></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/aa-g-20-2.png?raw=true" alt/></p><h3 id="High-Frequency,-n_{\\text{obs}}900"><a class="docs-heading-anchor" href="#High-Frequency,-n_{\\text{obs}}900">High Frequency, <span>$n_{\text{obs}}=900$</span></a><a id="High-Frequency,-n_{\\text{obs}}900-1"></a><a class="docs-heading-anchor-permalink" href="#High-Frequency,-n_{\\text{obs}}900" title="Permalink"></a></h3><p><strong>Function Evaluations</strong></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/fn-900-2.png?raw=true" alt/></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/aa-fn-900-2.png?raw=true" alt/></p><p><strong>Gradient Evaluations</strong></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/g-900-2.png?raw=true" alt/></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/Optimizers/aa-g-900-2.png?raw=true" alt/></p><p>We can see that LBFGS with a HagerZhang linesearch algorithm has the best performance, although it is very computationally expensive. Gradient descent methods without line search algorithms are not stable. However, if we apply Anderson Acceleration, the stability can be improved without additionaly function/gradient evaluations. </p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-cvx"><a class="tag is-link" href="#citeref-cvx">cvx</a>The backend of Anderson Acceleration is from <a href="https://github.com/cvxgrp/aa">here</a>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../parallel/">« Parallel Computing</a><a class="docs-footer-nextpage" href="../ode/">PDE/ODE Solvers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 8 August 2020 22:14">Saturday 8 August 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
