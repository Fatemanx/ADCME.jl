<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Bayesian Neural Networks · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../resources/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li class="is-active"><a class="tocitem" href>Bayesian Neural Networks</a><ul class="internal"><li><a class="tocitem" href="#Motivation-1"><span>Motivation</span></a></li><li><a class="tocitem" href="#Variational-Inference-1"><span>Variational Inference</span></a></li><li><a class="tocitem" href="#Parametric-Family-1"><span>Parametric Family</span></a></li><li><a class="tocitem" href="#Example-1"><span>Example</span></a></li></ul></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Deep Learning Schemes</a></li><li class="is-active"><a href>Bayesian Neural Networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Bayesian Neural Networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/bnn.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Bayesian-Neural-Networks-1"><a class="docs-heading-anchor" href="#Bayesian-Neural-Networks-1">Bayesian Neural Networks</a><a class="docs-heading-anchor-permalink" href="#Bayesian-Neural-Networks-1" title="Permalink"></a></h1><h2 id="Motivation-1"><a class="docs-heading-anchor" href="#Motivation-1">Motivation</a><a class="docs-heading-anchor-permalink" href="#Motivation-1" title="Permalink"></a></h2><p>Bayesian neural networks are different from plain neural networks in that weights and biases in Bayesian neural networks are interpreted in a probabilistic manner. Instead of finding a point estimation of weights and biases, in Bayesian neural networks, a prior distribution is assigned to the weights and biases, and a posterior distribution is obtained from the data. It relies on the Bayes formula </p><div>\[p(w|\mathcal{D}) = \frac{p(\mathcal{D}|w)p(w)}{p(\mathcal{D})}\]</div><p>Here <span>$\mathcal{D}$</span> is the data, e.g., the input-output pairs of the neural network <span>$\{(x_i, y_i)\}$</span>, <span>$w$</span> is the weights and biases of the neural network, and <span>$p(w)$</span> is the prior distribution. </p><p>If we have a full posterior distribution <span>$p(w|\mathcal{D})$</span>, we can conduct predictive modeling using </p><div>\[p(y|x, \mathcal{D}) = \int p(y|x, w) p(w|\mathcal{D})d w\]</div><p>However, computing <span>$p(w|\mathcal{D})$</span> is usually intractable since we need to compute the normalized factor <span>$p(\mathcal{D}) = \int p(\mathcal{D}|w)p(w) dw$</span>, which requires us to integrate over all possible <span>$w$</span>. Traditionally, Markov chain Monte Carlo (MCMC) has been used to sample from <span>$p(w|\mathcal{D})$</span> without evaluating <span>$p(\mathcal{D})$</span>. However, MCMC can converge very slowly and requires a voluminous number of sampling, which can be quite expensive. </p><h2 id="Variational-Inference-1"><a class="docs-heading-anchor" href="#Variational-Inference-1">Variational Inference</a><a class="docs-heading-anchor-permalink" href="#Variational-Inference-1" title="Permalink"></a></h2><p>In Bayesian neural networks, the idea is to approximate <span>$p(w|\mathcal{D})$</span> using a parametrized family <span>$p(w|\theta)$</span>, where <span>$\theta$</span> is the parameters. This method is called <strong>variational inference</strong>. We minimize the KL divergeence between the true posterior and the approximate posterial to find the optimal <span>$\theta$</span></p><div>\[\text{KL}(p(w|\theta)||p(w|\mathcal{D})) = \text{KL}(p(w|\theta)||p(W)) - \mathbb{E}_{p(w|\theta)}\log p(\mathcal{D}|w) + \log p(\mathcal{D})\]</div><p>Evaluating <span>$p(\mathcal{D})\geq 0$</span> is intractable, so we seek to minimize a lower bound of the KL divergence, which is known as <strong>variational free energy</strong></p><div>\[F(\mathcal{D}, \theta) =  \text{KL}(p(w|\theta)||p(w)) - \mathbb{E}_{p(w|\theta)}\log p(\mathcal{D}|w)\]</div><p>In practice, thee variational free energy is approximated by the discrete samples </p><div>\[F(\mathcal{D}, \theta) \approx  \frac{1}{N}\sum_{i=1}^N \left[\log p(w_i|\theta)) - \log p(w_i)  - \log p(\mathcal{D}|w_i)\right]\]</div><h2 id="Parametric-Family-1"><a class="docs-heading-anchor" href="#Parametric-Family-1">Parametric Family</a><a class="docs-heading-anchor-permalink" href="#Parametric-Family-1" title="Permalink"></a></h2><p>In Baysian neural networks, the parametric family is usually chosen to be the Gaussian distribution. For the sake of automatic differentiation, we usually parametrize <span>$w$</span> using </p><div>\[w = \mu + \sigma \otimes z\qquad z \sim \mathcal{N}(0, I) \tag{1}\]</div><p>Here <span>$\theta = (\mu, \sigma)$</span>. The prior distributions for <span>$\mu$</span> and <span>$\sigma$</span> are given as hyperparameters. For example, we can use a mixture of Gaussians as prior </p><div>\[\pi_1 \mathcal{N}(0, \sigma_1) + \pi_2 \mathcal{N}(0, \sigma_2)\]</div><p>The advantage of Equation 1 is that we can easily obtain the log probability <span>$\log p(w|\theta)$</span>. </p><p>Because <span>$\sigma$</span> should always be positive, we can instead parametrize another parameter <span>$\rho$</span> and transform <span>$\rho$</span> to <span>$\sigma$</span> using a softplus function </p><div>\[\sigma = \log (1+\exp(\rho))\]</div><h2 id="Example-1"><a class="docs-heading-anchor" href="#Example-1">Example</a><a class="docs-heading-anchor-permalink" href="#Example-1" title="Permalink"></a></h2><p>Now let us consider a concrete example. The following example is adapted from <a href="http://krasserm.github.io/2019/03/14/bayesian-neural-networks/">this post</a>. </p><h3 id="Generating-Training-Data-1"><a class="docs-heading-anchor" href="#Generating-Training-Data-1">Generating Training Data</a><a class="docs-heading-anchor-permalink" href="#Generating-Training-Data-1" title="Permalink"></a></h3><p>We first generate some 1D training data </p><pre><code class="language-julia">using ADCME
using PyPlot 
using ProgressMeter
using Statistics

function f(x, σ)
    ε = randn(size(x)...) * σ
    return 10 * sin.(2π*x) + ε
end

batch_size = 32
noise = 1.0

X = reshape(LinRange(-0.5, 0.5, batch_size)|&gt;Array, :, 1)
y = f(X, noise)
y_true = f(X, 0.0)

close(&quot;all&quot;)
scatter(X, y, marker=&quot;+&quot;, label=&quot;Training Data&quot;)
plot(X, y_true, label=&quot;Truth&quot;)
legend()</code></pre><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/bnn_training_data.png?raw=true" alt/></p><h3 id="Construct-Bayesian-Neural-Network-1"><a class="docs-heading-anchor" href="#Construct-Bayesian-Neural-Network-1">Construct Bayesian Neural Network</a><a class="docs-heading-anchor-permalink" href="#Construct-Bayesian-Neural-Network-1" title="Permalink"></a></h3><pre><code class="language-julia">
mutable struct VariationalLayer
    units
    activation
    prior_σ1
    prior_σ2
    prior_π1
    prior_π2
    Wμ
    bμ
    Wρ
    bρ
    init_σ
end

function VariationalLayer(units; activation=relu, prior_σ1=1.5, prior_σ2=0.1,
        prior_π1=0.5)
    init_σ = sqrt(
        prior_π1 * prior_σ1^2 + (1-prior_π1)*prior_σ2^2
    )
    VariationalLayer(units, activation, prior_σ1, prior_σ2, prior_π1, 1-prior_π1,
                        missing, missing, missing, missing, init_σ)
end

function kl_loss(vl, w, μ, σ)
    dist = ADCME.Normal(μ,σ)
    return sum(logpdf(dist, w)-logprior(vl, w))
end

function logprior(vl, w)
    dist1 = ADCME.Normal(constant(0.0), vl.prior_σ1)
    dist2 = ADCME.Normal(constant(0.0), vl.prior_σ2)
    log(vl.prior_π1*exp(logpdf(dist1, w)) + vl.prior_π2*exp(logpdf(dist2, w)))
end

function (vl::VariationalLayer)(x)
    x = constant(x)
    if ismissing(vl.bμ)
        vl.Wμ = get_variable(vl.init_σ*randn(size(x,2), vl.units))
        vl.Wρ = get_variable(zeros(size(x,2), vl.units))
        vl.bμ = get_variable(vl.init_σ*randn(1, vl.units))
        vl.bρ = get_variable(zeros(1, vl.units))
    end
    Wσ = softplus(vl.Wρ)
    W = vl.Wμ + Wσ.*normal(size(vl.Wμ)...) 
    bσ = softplus(vl.bρ)
    b = vl.bμ + bσ.*normal(size(vl.bμ)...)
    loss = kl_loss(vl, W, vl.Wμ, Wσ) + kl_loss(vl, b, vl.bμ, bσ)
    out = vl.activation(x * W + b)
    return out, loss 
end

function neg_log_likelihood(y_obs, y_pred, σ)
    y_obs = constant(y_obs)
    dist = ADCME.Normal(y_pred, σ)
    sum(-logpdf(dist, y_obs))
end

ipt = placeholder(X)
x, loss1 = VariationalLayer(20, activation=relu)(ipt)
x, loss2 = VariationalLayer(20, activation=relu)(x)
x, loss3 = VariationalLayer(1, activation=x-&gt;x)(x)

loss_lf = neg_log_likelihood(y, x, noise)
loss = loss1 + loss2 + loss3 + loss_lf</code></pre><h3 id="Optimization-1"><a class="docs-heading-anchor" href="#Optimization-1">Optimization</a><a class="docs-heading-anchor-permalink" href="#Optimization-1" title="Permalink"></a></h3><p>We use an ADAM optimizer to optimize the loss function. In this case, quasi-Newton methods that are typically used for deterministic function optimization are not appropriate because the loss function essentially involves stochasticity. </p><p>Another caveat is that because the neural network may have many local minimum, we need to run the optimizer multiple times in order to obtain a good local minimum. </p><pre><code class="language-julia">
opt = AdamOptimizer(0.08).minimize(loss)
sess = Session(); init(sess)
@showprogress for i = 1:5000
    run(sess, opt)
end


X_test = reshape(LinRange(-1.5,1.5,32)|&gt;Array, :, 1)
y_pred_list = []
@showprogress for i = 1:10000
    y_pred = run(sess, x, ipt=&gt;X_test)
    push!(y_pred_list, y_pred)
end

y_preds = hcat(y_pred_list...)

y_mean = mean(y_preds, dims=2)[:]
y_std = std(y_preds, dims=2)[:]

close(&quot;all&quot;)
plot(X_test, y_mean)
scatter(X[:], y[:], marker=&quot;+&quot;)
fill_between(X_test[:], y_mean-2y_std, y_mean+2y_std, alpha=0.5)</code></pre><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/bnn_prediction.png?raw=true" alt/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../convnet/">« Convolutional Neural Network</a><a class="docs-footer-nextpage" href="../apps/">Overview »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 26 May 2020 17:16">Tuesday 26 May 2020</span>. Using Julia version 1.4.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
