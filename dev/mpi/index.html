<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Using MPI with ADCME · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../resources/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../smt/">Managing Numerical Experiments with SMT</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li class="is-active"><a class="tocitem" href>Using MPI with ADCME</a><ul class="internal"><li><a class="tocitem" href="#A-First-Example"><span>A First Example</span></a></li><li><a class="tocitem" href="#Custom-Operators-with-MPI"><span>Custom Operators with MPI</span></a></li><li><a class="tocitem" href="#MPI-Configuration-in-CMakeLists.txt"><span>MPI Configuration in CMakeLists.txt</span></a></li></ul></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Resources</a></li><li class="is-active"><a href>Using MPI with ADCME</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Using MPI with ADCME</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/mpi.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Using-MPI-with-ADCME"><a class="docs-heading-anchor" href="#Using-MPI-with-ADCME">Using MPI with ADCME</a><a id="Using-MPI-with-ADCME-1"></a><a class="docs-heading-anchor-permalink" href="#Using-MPI-with-ADCME" title="Permalink"></a></h1><p>Many large-scale scientific computing involves parallel computing. Among many parallel computing models, the MPI  is one of the most popular models. In this section, we describe how ADCME can work with MPI for solving inverse modeling. Specifically, we describe how gradients can be back-propagated via MPI function calls.  </p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Message Passing Interface (MPI) is an interface for parallel computing based on message passing models. In the message passing model, a master process assigns work to workers by passing them a message that describes the work. The message may be data or meta information (e.g., operations to perform). A consensus was reached around 1992 and the MPI standard was born. MPI is a definition of interface, and the implementations are left to hardware venders. </p></div></div><h2 id="A-First-Example"><a class="docs-heading-anchor" href="#A-First-Example">A First Example</a><a id="A-First-Example-1"></a><a class="docs-heading-anchor-permalink" href="#A-First-Example" title="Permalink"></a></h2><p>We will utilize the <a href="https://github.com/JuliaParallel/MPI.jl">MPI.jl</a> package, which is a Julia interface to many MPI implementations (MPICH, MSMPI, etc.). Because MPI.jl is essentially a wrapper of MPI C language implementations, we can freely interact shared library with MPI.jl. For example, in <a href="docs/src/assets/Codes/mpi">this directory</a>, we have a C++ code </p><pre><code class="language-c">#include &lt;mpi.h&gt;
#include &lt;iostream&gt;

#ifdef _WIN32
#define EXPORTED __declspec(dllexport)
#else
#define EXPORTED
#endif 

extern &quot;C&quot; EXPORTED int printinfo(){
    MPI_Comm comm = MPI_COMM_WORLD;
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);

    char processor_name[1000];
    int name_len;
    MPI_Get_processor_name( processor_name , &amp;name_len);
    printf(&quot;Hello world from processor %s, rank %d out of %d processors\n&quot;,
        processor_name, world_rank, world_size);
    int result;
    MPI_Reduce( &amp;world_rank , &amp;result , 1 , MPI_INT , MPI_SUM , 0 , comm);
    return result;
}</code></pre><p>After we compile it into a shared library, we can write Julia codes that mix the C++ kernel and MPI.jl</p><pre><code class="language-julia">using MPI 

MPI.Init()

v = ccall((:printinfo, &quot;./build/debug/mtest.dll&quot;), Cint, ())
print(v)</code></pre><p>For example, in the shell we type </p><pre><code class="language-bash">mpiexec.exe -n 4 julia test.jl</code></pre><p>We have the following output </p><pre><code class="language-none">13296304
13296304
13296304
6
Hello world from processor LAPTOP-92GNG3GT.stanford.edu, rank 1 out of 4 processors
Hello world from processor LAPTOP-92GNG3GT.stanford.edu, rank 2 out of 4 processors
Hello world from processor LAPTOP-92GNG3GT.stanford.edu, rank 0 out of 4 processors
Hello world from processor LAPTOP-92GNG3GT.stanford.edu, rank 3 out of 4 processors</code></pre><p>The first three numbers are junk because they are on the worker processors. </p><h2 id="Custom-Operators-with-MPI"><a class="docs-heading-anchor" href="#Custom-Operators-with-MPI">Custom Operators with MPI</a><a id="Custom-Operators-with-MPI-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-Operators-with-MPI" title="Permalink"></a></h2><p>We can also make custom operators with MPI. Let us consider computing</p><div>\[f(\theta) = \sum_{i=1}^n f_i(\theta)\]</div><p>Each <span>$f_i$</span> is a very expensive function so it makes sense to use MPI to split the jobs on different processors. To simplify the problem, we consider </p><div>\[f(\theta) = f_1(\theta) + f_2(\theta) + f_3(\theta) + f_4(\theta)\]</div><p>where <span>$f_i(\theta) = \theta^{i-1}$</span>. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi.png?raw=true" alt/></p><p>We make two custom operators: <code>basis</code> </p><div>\[\texttt{basis}(\theta) = 1, \theta, \theta^2, \theta^3, \ldots, \theta^n\]</div><p>and <code>m_sum</code></p><div>\[\texttt{m\_sum}(a_1, a_2, \ldots, a_n) = a_1 + a_2 + \ldots +a_n\]</div><p>Here <span>$n$</span> is the number of processors. </p><p>To implement <code>basis</code>, we need to broadcast <span>$\theta$</span> at processor 0 to <span>$n$</span> processors (<code>MPI_Bcast</code>). For back-propagating the gradients, for each processor, we need to back-propagate the gradient, respectively, and then aggregate the gradients on processor 0 (<code>MPI_Reduce</code>). This leads to the following forward and backward implementation:</p><pre><code class="language-c">#include &quot;mpi.h&quot;
#include &lt;iostream&gt; 


void forward(double *c,  const double *a){
    MPI_Comm comm = MPI_COMM_WORLD;
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);
    if(world_rank==0)
      c[0] = a[0];
    MPI_Barrier( comm);
    MPI_Bcast( c , 1 , MPI_DOUBLE , 0 , comm);
    c[0] = pow(c[0], world_rank);
}

void backward(
  double *grad_a,
  const double *grad_c,
  const double *c, const double *a){
    MPI_Comm comm = MPI_COMM_WORLD;
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);
    if(world_rank==0)
      grad_a[0] = 0.0;
    else
      grad_a[0] = grad_c[0] * world_rank * pow(a[0], world_rank-1);
    double ga;
    MPI_Reduce( grad_a , &amp;ga , 1 , MPI_DOUBLE , MPI_SUM , 0 , comm);
    if(world_rank==0){
      grad_a[0] = ga;
    }
      
}</code></pre><p>For implementing <code>m_sum</code>, the forward is obvious an <code>MPI_Reduce</code> function call. For the backward, we need to back-propagate the scalar gradient to each input, and this procedure requires an <code>MPI_Bcast</code> function call. </p><pre><code class="language-c">#include &quot;mpi.h&quot;


void forward(double *b, const double *a){
   MPI_Comm comm = MPI_COMM_WORLD;
    MPI_Reduce( a , b , 1 , MPI_DOUBLE , MPI_SUM , 0 , comm);
}

void backward(
    double *grad_a, const double *grad_b){
    MPI_Comm comm = MPI_COMM_WORLD;
    grad_a[0] = grad_b[0];
    MPI_Bcast( grad_a , 1 , MPI_DOUBLE , 0 , comm);
}</code></pre><p>The custom operators are elegantly integrated with ADCME.jl and MPI.jl:</p><pre><code class="language-julia">using MPI 
using ADCME

# interfaces to custom operators 
function basis(a)
    basis_ = load_op_and_grad(&quot;./Basis/build/libBasis&quot;,&quot;basis&quot;)
    a = convert_to_tensor(Any[a], [Float64]); a = a[1]
    basis_(a)
end

function m_sum(a)
    m_sum_ = load_op_and_grad(&quot;./Sum/build/libMSum&quot;,&quot;m_sum&quot;)
    a = convert_to_tensor(Any[a], [Float64]); a = a[1]
    m_sum_(a)
end

MPI.Init()
a = constant(2.0)
b = basis(a)
c = m_sum(b)
g = gradients(c, a)

sess = Session(); init(sess)
result = run(sess, c)
grad = run(sess, g)


if MPI.Comm_rank(MPI.COMM_WORLD)==0
    @info result, grad
end</code></pre><p>To run the code, in the shell, we type</p><pre><code class="language-bash">mpiexec.exe -n 4 julia test_mpi.jl</code></pre><p>We obtain the outputs as expected:</p><pre><code class="language-none">[ Info: (15.0, 17.0)</code></pre><h2 id="MPI-Configuration-in-CMakeLists.txt"><a class="docs-heading-anchor" href="#MPI-Configuration-in-CMakeLists.txt">MPI Configuration in CMakeLists.txt</a><a id="MPI-Configuration-in-CMakeLists.txt-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-Configuration-in-CMakeLists.txt" title="Permalink"></a></h2><p>To configure MPI in <code>CMakeLists.txt</code>, we can simply add the following commands</p><pre><code class="language-cmakelists">find_package(MPI)
include_directories(SYSTEM ${MPI_INCLUDE_PATH})
target_link_libraries(mylib ${MPI_C_LIBRARIES})</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mcmc/">« Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a><a class="docs-footer-nextpage" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 8 July 2020 20:21">Wednesday 8 July 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
