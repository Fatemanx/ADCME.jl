<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../resources/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Core-Functions-1"><span>Core Functions</span></a></li><li><a class="tocitem" href="#Variables-1"><span>Variables</span></a></li><li><a class="tocitem" href="#Random-Variables-1"><span>Random Variables</span></a></li><li><a class="tocitem" href="#Sparse-Matrix-1"><span>Sparse Matrix</span></a></li><li><a class="tocitem" href="#Operations-1"><span>Operations</span></a></li><li><a class="tocitem" href="#IO-1"><span>IO</span></a></li><li><a class="tocitem" href="#Optimization-1"><span>Optimization</span></a></li><li><a class="tocitem" href="#Neural-Networks-1"><span>Neural Networks</span></a></li><li><a class="tocitem" href="#Generative-Neural-Nets-1"><span>Generative Neural Nets</span></a></li><li><a class="tocitem" href="#Tools-1"><span>Tools</span></a></li><li><a class="tocitem" href="#ODE-1"><span>ODE</span></a></li><li><a class="tocitem" href="#Optimal-Transport-1"><span>Optimal Transport</span></a></li><li><a class="tocitem" href="#Misc-1"><span>Misc</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference-1"><a class="docs-heading-anchor" href="#API-Reference-1">API Reference</a><a class="docs-heading-anchor-permalink" href="#API-Reference-1" title="Permalink"></a></h1><h2 id="Core-Functions-1"><a class="docs-heading-anchor" href="#Core-Functions-1">Core Functions</a><a class="docs-heading-anchor-permalink" href="#Core-Functions-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.control_dependencies-Tuple{Any,Union{Tuple, PyCall.PyObject, Array{PyCall.PyObject,N} where N}}" href="#ADCME.control_dependencies-Tuple{Any,Union{Tuple, PyCall.PyObject, Array{PyCall.PyObject,N} where N}}"><code>ADCME.control_dependencies</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">control_dependencies(f, ops::Union{Array{PyObject}, PyObject})</code></pre><p>Executes all operations in <code>ops</code> before any operations <em>created</em> inside the block. </p><pre><code class="language-julia">op1 = tf.print(&quot;print op1&quot;)
op3 = tf.print(&quot;print op3&quot;)
control_dependencies(op1) do
    global op2 = tf.print(&quot;print op2&quot;)
end
run(sess, [op2,op3])</code></pre><p>In this example, <code>op1</code> must be executed before <code>op2</code>. But there is no guarantee when <code>op3</code> will be executed.  There are several possible outputs of the program such as</p><pre><code class="language-julia-repl">print op3
print op1
print op2</code></pre><p>or </p><pre><code class="language-none">print op1
print op3
print op2</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/core.jl#L97-L123">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.get_collection" href="#ADCME.get_collection"><code>ADCME.get_collection</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">get_collection(name::Union{String, Missing})</code></pre><p>Returns the collection with name <code>name</code>. If <code>name</code> is <code>missing</code>, returns all the trainable variables.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/core.jl#L29-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.has_gpu-Tuple{}" href="#ADCME.has_gpu-Tuple{}"><code>ADCME.has_gpu</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">has_gpu()</code></pre><p>Checks if GPU is available.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>ADCME will use GPU automatically if GPU is available. To disable GPU, set the environment variable <code>ENV[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;&quot;</code> before importing ADCME </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/core.jl#L240-L247">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.if_else-Tuple{Union{Bool, PyCall.PyObject, Array},Any,Any,Vararg{Any,N} where N}" href="#ADCME.if_else-Tuple{Union{Bool, PyCall.PyObject, Array},Any,Any,Vararg{Any,N} where N}"><code>ADCME.if_else</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">if_else(condition::Union{PyObject,Array,Bool}, fn1, fn2, args...;kwargs...)</code></pre><ul><li>If <code>condition</code> is a scalar boolean, it outputs <code>fn1</code> or <code>fn2</code> (a function with no input argument or a tensor) based on whether <code>condition</code> is true or false.</li><li>If <code>condition</code> is a boolean array, if returns <code>condition .* fn1 + (1 - condition) .* fn2</code></li></ul><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>If you encounter an error like this:</p><pre><code class="language-none">tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value</code></pre><p>It&#39;s probably that your code within <code>if_else</code> is not valid. </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/core.jl#L216-L228">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.independent-Tuple{PyCall.PyObject,Vararg{Any,N} where N}" href="#ADCME.independent-Tuple{PyCall.PyObject,Vararg{Any,N} where N}"><code>ADCME.independent</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">independent(o::PyObject, args...; kwargs...)</code></pre><p>Returns <code>o</code> but when computing the gradients, the top gradients will not be back-propagated into dependent variables of <code>o</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/core.jl#L257-L261">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.reset_default_graph-Tuple{}" href="#ADCME.reset_default_graph-Tuple{}"><code>ADCME.reset_default_graph</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">reset_default_graph()</code></pre><p>Resets the graph by removing all the operators. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/core.jl#L18-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.tensor-Tuple{String}" href="#ADCME.tensor-Tuple{String}"><code>ADCME.tensor</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">tensor(s::String)</code></pre><p>Returns the tensor with name <code>s</code>. See <a href="#ADCME.tensorname-Tuple{PyCall.PyObject}"><code>tensorname</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/core.jl#L54-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.tensorname-Tuple{PyCall.PyObject}" href="#ADCME.tensorname-Tuple{PyCall.PyObject}"><code>ADCME.tensorname</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">tensorname(o::PyObject)</code></pre><p>Returns the name of the tensor. See <a href="#ADCME.tensor-Tuple{String}"><code>tensor</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/core.jl#L63-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.while_loop-Tuple{Union{Function, PyCall.PyObject},Function,Union{PyCall.PyObject, Array{Any,N} where N, Array{PyCall.PyObject,N} where N}}" href="#ADCME.while_loop-Tuple{Union{Function, PyCall.PyObject},Function,Union{PyCall.PyObject, Array{Any,N} where N, Array{PyCall.PyObject,N} where N}}"><code>ADCME.while_loop</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">while_loop(condition::Union{PyObject,Function}, body::Function, loop_vars::Union{PyObject, Array{Any}, Array{PyObject}};
    parallel_iterations::Int64=10, kwargs...)</code></pre><p>Loops over <code>loop_vars</code> while <code>condition</code> is true. This operator only creates one extra node to mark the loops in the computational graph.</p><p><strong>Example</strong></p><p>The following script computes </p><div>\[\sum_{i=1}^{10} i\]</div><pre><code class="language-julia">function condition(i, ta)
    i &lt;= 10
end
function body(i, ta)
    u = read(ta, i-1)
    ta = write(ta, i, u+1)
    i+1, ta
end
ta = TensorArray(10)
ta = write(ta, 1, constant(1.0))
i = constant(2, dtype=Int32)
_, out = while_loop(condition, body, [i, ta])
summation = stack(out)[10]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/core.jl#L152-L180">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.run_profile-Tuple" href="#ADCME.run_profile-Tuple"><code>ADCME.run_profile</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">run_profile(args...;kwargs...)</code></pre><p>Runs the session with tracing information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/run.jl#L32-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.save_profile" href="#ADCME.save_profile"><code>ADCME.save_profile</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">save_profile(filename::String=&quot;default_timeline.json&quot;)</code></pre><p>Save the timeline information to file <code>filename</code>. </p><ul><li>Open Chrome and navigate to chrome://tracing</li><li>Load the timeline file</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/run.jl#L44-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.bind-Tuple{PyCall.PyObject,Vararg{Any,N} where N}" href="#Base.bind-Tuple{PyCall.PyObject,Vararg{Any,N} where N}"><code>Base.bind</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">bind(op::PyObject, ops...)</code></pre><p>Adding operations <code>ops</code> to the dependencies of <code>op</code>. The function is useful when we want to execute <code>ops</code> but <code>ops</code> is not  in the dependency of the final output. For example, if we want to print <code>i</code> each time <code>i</code> is evaluated</p><pre><code class="language-julia">i = constant(1.0)
op = tf.print(i)
i = bind(i, op)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/core.jl#L133-L143">source</a></section></article><h2 id="Variables-1"><a class="docs-heading-anchor" href="#Variables-1">Variables</a><a class="docs-heading-anchor-permalink" href="#Variables-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.TensorArray" href="#ADCME.TensorArray"><code>ADCME.TensorArray</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">TensorArray(size_::Int64=0, args...;kwargs...)</code></pre><p>Constructs a tensor array for <a href="#ADCME.while_loop-Tuple{Union{Function, PyCall.PyObject},Function,Union{PyCall.PyObject, Array{Any,N} where N, Array{PyCall.PyObject,N} where N}}"><code>while_loop</code></a>.  </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L590-L594">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.Variable-Tuple{Any}" href="#ADCME.Variable-Tuple{Any}"><code>ADCME.Variable</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Variable(initial_value;kwargs...)</code></pre><p>Constructs a trainable tensor from <code>value</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L50-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.cell-Tuple{Array,Vararg{Any,N} where N}" href="#ADCME.cell-Tuple{Array,Vararg{Any,N} where N}"><code>ADCME.cell</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">cell(arr::Array, args...;kwargs...)</code></pre><p>Construct a cell tensor. </p><p><strong>Example</strong></p><pre><code class="language-julia-REPL">julia&gt; r = cell([[1.],[2.,3.]])
julia&gt; run(sess, r[1])
1-element Array{Float32,1}:
 1.0
julia&gt; run(sess, r[2])
2-element Array{Float32,1}:
 2.0
 3.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L63-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.constant-Tuple{Any}" href="#ADCME.constant-Tuple{Any}"><code>ADCME.constant</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">constant(value; kwargs...)</code></pre><p>Constructs a non-trainable tensor from <code>value</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L31-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.convert_to_tensor-Union{Tuple{Union{Missing, Nothing, Number, PyCall.PyObject, Array{T,N} where N}}, Tuple{T}} where T&lt;:Number" href="#ADCME.convert_to_tensor-Union{Tuple{Union{Missing, Nothing, Number, PyCall.PyObject, Array{T,N} where N}}, Tuple{T}} where T&lt;:Number"><code>ADCME.convert_to_tensor</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">convert_to_tensor(o::Union{PyObject, Number, Array{T}, Missing, Nothing}; dtype::Union{Type, Missing}=missing) where T&lt;:Number
convert_to_tensor(os::Array, dtypes::Array)</code></pre><p>Converts the input <code>o</code> to tensor. If <code>o</code> is already a tensor and <code>dtype</code> (if provided) is the same as that of <code>o</code>, the operator does nothing. Otherwise, <code>convert_to_tensor</code> converts the numerical array to a constant tensor or casts the data type. <code>convert_to_tensor</code> also accepts multiple tensors. </p><p><strong>Example</strong></p><pre><code class="language-julia">convert_to_tensor([1.0, constant(rand(2)), rand(10)], [Float32, Float64, Float32])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L630-L642">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.get_variable-Tuple{Union{Number, PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Number}}" href="#ADCME.get_variable-Tuple{Union{Number, PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Number}}"><code>ADCME.get_variable</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_variable(o::Union{PyObject, Bool, Array{&lt;:Number}}; 
    name::Union{String, Missing} = missing, 
    scope::String = &quot;&quot;)</code></pre><p>Creates a new variable with initial value <code>o</code>. If <code>name</code> exists, <code>get_variable</code> returns the variable instead of create a new one.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L105-L111">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.get_variable-Union{Tuple{Type}, Tuple{N}} where N" href="#ADCME.get_variable-Union{Tuple{Type}, Tuple{N}} where N"><code>ADCME.get_variable</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_variable(dtype::Type;
shape::Union{Array{&lt;:Integer}, NTuple{N, &lt;:Integer}}, 
name::Union{Missing,String} = missing
scope::String = &quot;&quot;)</code></pre><p>Creates a new variable with initial value <code>o</code>. If <code>name</code> exists, <code>get_variable</code> returns the variable instead of create a new one.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L127-L134">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.gradient_checkpointing" href="#ADCME.gradient_checkpointing"><code>ADCME.gradient_checkpointing</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">gradient_checkpointing(type::String=&quot;speed&quot;)</code></pre><p>Uses checkpointing scheme for gradients. </p><ul><li>&#39;speed&#39;:  checkpoint all outputs of convolutions and matmuls. these ops are usually the most expensive,   so checkpointing them maximizes the running speed   (this is a good option if nonlinearities, concats, batchnorms, etc are taking up a lot of memory)</li><li>&#39;memory&#39;: try to minimize the memory usage   (currently using a very simple strategy that identifies a number of bottleneck tensors in the graph to checkpoint)</li><li>&#39;collection&#39;: look for a tensorflow collection named &#39;checkpoints&#39;, which holds the tensors to checkpoint</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L668-L678">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.gradient_magnitude-Tuple{PyCall.PyObject,Union{PyCall.PyObject, Array}}" href="#ADCME.gradient_magnitude-Tuple{PyCall.PyObject,Union{PyCall.PyObject, Array}}"><code>ADCME.gradient_magnitude</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">gradient_magnitude(l::PyObject, o::Union{Array, PyObject})</code></pre><p>Returns the gradient sum </p><div>\[\sqrt{\sum_{i=1}^n \|\frac{\partial l}{\partial o_i}\|^2}\]</div><p>This function is useful for debugging the training</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L702-L710">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.gradients-Tuple{PyCall.PyObject,PyCall.PyObject}" href="#ADCME.gradients-Tuple{PyCall.PyObject,PyCall.PyObject}"><code>ADCME.gradients</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">gradients(ys::PyObject, xs::PyObject; kwargs...)</code></pre><p>Computes the gradients of <code>ys</code> w.r.t <code>xs</code>. </p><ul><li>If <code>ys</code> is a scalar, <code>gradients</code> returns the gradients with the same shape as <code>xs</code>.</li><li>If <code>ys</code> is a vector, <code>gradients</code> returns the Jacobian <span>$\frac{\partial y}{\partial x}$</span></li></ul><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The second usage is not suggested since <code>ADCME</code> adopts reverse mode automatic differentiation.  Although in the case <code>ys</code> is a vector and <code>xs</code> is a scalar, <code>gradients</code> cleverly uses forward mode automatic differentiation, it requires that the second order gradients are implemented for relevant operators. </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L247-L259">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.hessian-Tuple{PyCall.PyObject,PyCall.PyObject}" href="#ADCME.hessian-Tuple{PyCall.PyObject,PyCall.PyObject}"><code>ADCME.hessian</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>hessian</code> computes the hessian of a scalar function f with respect to vector inputs xs</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L358-L360">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.ones_like-Tuple{Union{Real, PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Real},Vararg{Any,N} where N}" href="#ADCME.ones_like-Tuple{Union{Real, PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Real},Vararg{Any,N} where N}"><code>ADCME.ones_like</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ones_like(o::Union{PyObject,Real, Array{&lt;:Real}}, args...; kwargs...)</code></pre><p>Returns a all-one tensor, which has the same size as <code>o</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia">a = rand(100,10)
b = ones_like(a)
@assert run(sess, b)≈ones(100,10)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L735-L746">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.placeholder-Tuple{Type}" href="#ADCME.placeholder-Tuple{Type}"><code>ADCME.placeholder</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">placeholder(dtype::Type; kwargs...)</code></pre><p>Creates a placeholder of the type <code>dtype</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia">a = placeholder(Float64, shape=[20,10])
b = placeholder(Float64, shape=[]) # a scalar 
c = placeholder(Float64, shape=[nothing]) # a vector</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L155-L165">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.placeholder-Tuple{Union{Number, PyCall.PyObject, Array}}" href="#ADCME.placeholder-Tuple{Union{Number, PyCall.PyObject, Array}}"><code>ADCME.placeholder</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">placeholder(o::Union{Number, Array, PyObject}; kwargs...)</code></pre><p>Creates a placeholder of the same type and size as <code>o</code>. <code>o</code> is the default value. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L175-L179">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.tensor-Union{Tuple{Array{T,1}}, Tuple{T}} where T" href="#ADCME.tensor-Union{Tuple{Array{T,1}}, Tuple{T}} where T"><code>ADCME.tensor</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">tensor(v::Array{T,2}; dtype=Float64, sparse=false) where T</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L540-L542">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.tensor-Union{Tuple{Array{T,2}}, Tuple{T}} where T" href="#ADCME.tensor-Union{Tuple{Array{T,2}}, Tuple{T}} where T"><code>ADCME.tensor</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">tensor(v::Array{T,2}; dtype=Float64, sparse=false) where T</code></pre><p>Convert a generic array <code>v</code> to a tensor. For example, </p><pre><code class="language-julia">v = [0.0 constant(1.0) 2.0
    constant(2.0) 0.0 1.0]
u = tensor(v)</code></pre><p><code>u</code> will be a <span>$2\times 3$</span> tensor. </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This function is expensive. Use with caution.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L559-L571">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.zeros_like-Tuple{Union{Real, PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Real},Vararg{Any,N} where N}" href="#ADCME.zeros_like-Tuple{Union{Real, PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Real},Vararg{Any,N} where N}"><code>ADCME.zeros_like</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">zeros_like(o::Union{PyObject,Real, Array{&lt;:Real}}, args...; kwargs...)</code></pre><p>Returns a all-zero tensor, which has the same size as <code>o</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia">a = rand(100,10)
b = zeros_like(a)
@assert run(sess, b)≈zeros(100,10)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L718-L729">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.copy-Tuple{PyCall.PyObject}" href="#Base.copy-Tuple{PyCall.PyObject}"><code>Base.copy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">copy(o::PyObject)</code></pre><p>Creates a tensor that has the same value that is currently stored in a variable.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The output is a graph node that will have that value when evaluated. Any time you evaluate it, it will grab the current value of <code>o</code>. </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L87-L94">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.read-Tuple{PyCall.PyObject,Union{Integer, PyCall.PyObject}}" href="#Base.read-Tuple{PyCall.PyObject,Union{Integer, PyCall.PyObject}}"><code>Base.read</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">read(ta::PyObject, i::Union{PyObject,Integer})</code></pre><p>Reads data from <a href="#ADCME.TensorArray"><code>TensorArray</code></a> at index <code>i</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L610-L615">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.write-Tuple{PyCall.PyObject,Union{Integer, PyCall.PyObject},PyCall.PyObject}" href="#Base.write-Tuple{PyCall.PyObject,Union{Integer, PyCall.PyObject},PyCall.PyObject}"><code>Base.write</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">write(ta::PyObject, i::Union{PyObject,Integer}, obj)</code></pre><p>Writes data <code>obj</code> to <a href="#ADCME.TensorArray"><code>TensorArray</code></a> at index <code>i</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/variable.jl#L619-L624">source</a></section></article><h2 id="Random-Variables-1"><a class="docs-heading-anchor" href="#Random-Variables-1">Random Variables</a><a class="docs-heading-anchor-permalink" href="#Random-Variables-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.categorical-Tuple{Union{Integer, PyCall.PyObject}}" href="#ADCME.categorical-Tuple{Union{Integer, PyCall.PyObject}}"><code>ADCME.categorical</code></a> — <span class="docstring-category">Method</span></header><section><div><p>categorical(n::Union{PyObject, Integer}; kwargs...)</p><p><code>kwargs</code> has a keyword argument <code>logits</code>, a 2-D Tensor with shape <code>[batch_size, num_classes]</code>.   Each slice <code>[i, :]</code> represents the unnormalized log-probabilities for all classes.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/random.jl#L16-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.choice-Tuple{Union{PyCall.PyObject, Array},Union{Integer, PyCall.PyObject}}" href="#ADCME.choice-Tuple{Union{PyCall.PyObject, Array},Union{Integer, PyCall.PyObject}}"><code>ADCME.choice</code></a> — <span class="docstring-category">Method</span></header><section><div><p>choice(inputs::Union{PyObject, Array}, n_samples::Union{PyObject, Integer};replace::Bool=false)</p><p>Choose <code>n_samples</code> samples from <code>inputs</code> with/without replacement. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/random.jl#L41-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.logpdf-Union{Tuple{T}, Tuple{T,Any}} where T&lt;:ADCME.ADCMEDistribution" href="#ADCME.logpdf-Union{Tuple{T}, Tuple{T,Any}} where T&lt;:ADCME.ADCMEDistribution"><code>ADCME.logpdf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">logpdf(dist::T, x) where T&lt;:ADCMEDistribution</code></pre><p>Returns the log(prob) for a distribution <code>dist</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/random.jl#L112-L116">source</a></section></article><h2 id="Sparse-Matrix-1"><a class="docs-heading-anchor" href="#Sparse-Matrix-1">Sparse Matrix</a><a class="docs-heading-anchor-permalink" href="#Sparse-Matrix-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.SparseTensor" href="#ADCME.SparseTensor"><code>ADCME.SparseTensor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">SparseTensor</code></pre><p>A sparse matrix object. It has two fields </p><ul><li><p><code>o</code>: internal data structure </p></li><li><p><code>_diag</code>: <code>true</code> if the sparse matrix is marked as &quot;diagonal&quot;.</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L8-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.SparseTensor-Tuple{SparseArrays.SparseMatrixCSC}" href="#ADCME.SparseTensor-Tuple{SparseArrays.SparseMatrixCSC}"><code>ADCME.SparseTensor</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">SparseTensor(A::SparseMatrixCSC)
SparseTensor(A::Array{Float64, 2})</code></pre><p>Creates a <code>SparseTensor</code> from numerical arrays. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L108-L113">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.SparseTensor-Union{Tuple{S}, Tuple{T}, Tuple{Union{Array{T,1}, PyCall.PyObject},Union{Array{T,1}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject}}, Tuple{Union{Array{T,1}, PyCall.PyObject},Union{Array{T,1}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject},Union{Nothing, PyCall.PyObject, S}}, Tuple{Union{Array{T,1}, PyCall.PyObject},Union{Array{T,1}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject},Union{Nothing, PyCall.PyObject, S},Union{Nothing, PyCall.PyObject, S}}} where S&lt;:Integer where T&lt;:Integer" href="#ADCME.SparseTensor-Union{Tuple{S}, Tuple{T}, Tuple{Union{Array{T,1}, PyCall.PyObject},Union{Array{T,1}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject}}, Tuple{Union{Array{T,1}, PyCall.PyObject},Union{Array{T,1}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject},Union{Nothing, PyCall.PyObject, S}}, Tuple{Union{Array{T,1}, PyCall.PyObject},Union{Array{T,1}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject},Union{Nothing, PyCall.PyObject, S},Union{Nothing, PyCall.PyObject, S}}} where S&lt;:Integer where T&lt;:Integer"><code>ADCME.SparseTensor</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">SparseTensor(I::Union{PyObject,Array{T,1}}, J::Union{PyObject,Array{T,1}}, V::Union{Array{Float64,1}, PyObject}, m::Union{S, PyObject, Nothing}=nothing, n::Union{S, PyObject, Nothing}=nothing) where {T&lt;:Integer, S&lt;:Integer}</code></pre><p>Constructs a sparse tensor.  Examples:</p><pre><code class="language-none">ii = [1;2;3;4]
jj = [1;2;3;4]
vv = [1.0;1.0;1.0;1.0]
s = SparseTensor(ii, jj, vv, 4, 4)
s = SparseTensor(sprand(10,10,0.3))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L38-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.SparseAssembler" href="#ADCME.SparseAssembler"><code>ADCME.SparseAssembler</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">SparseAssembler(handle::Union{PyObject, &lt;:Integer}, n::Union{PyObject, &lt;:Integer}, tol::Union{PyObject, &lt;:Real}=0.0)</code></pre><p>Creates a SparseAssembler for accumulating <code>row</code>, <code>col</code>, <code>val</code> for sparse matrices. </p><ul><li><code>handle</code>: an integer handle for creating a sparse matrix. If the handle already exists, <code>SparseAssembler</code> return the existing sparse matrix handle. If you are creating different sparse matrices, the handles should be different. </li><li><code>n</code>: Number of rows of the sparse matrix. </li><li><code>tol</code> (optional): Tolerance. <code>SparseAssembler</code> will treats any values less than <code>tol</code> as zero. </li></ul><p><strong>Example 1</strong></p><pre><code class="language-julia">handle = SparseAssembler(100, 5, 1e-8)
op1 = accumulate(handle, 1, [1;2;3], [1.0;2.0;3.0])
op2 = accumulate(handle, 2, [1;2;3], [1.0;2.0;3.0])
J = assemble(5, 5, [op1;op2])</code></pre><p><code>J</code> will be a <a href="../api/#ADCME.SparseTensor"><code>SparseTensor</code></a> object. </p><p><strong>Example 2</strong></p><pre><code class="language-julia">handle = SparseAssembler(0, 5)
op1 = accumulate(handle, 1, [1;2;3], ones(3))
op2 = accumulate(handle, 1, [3], [1.])
op3 = accumulate(handle, 2, [1;3], ones(2))
J = assemble(5, 5, [op1;op2;op3]) # op1, op2, op3 are parallel
Array(run(sess, J))≈[1.0  1.0  2.0  0.0  0.0
                1.0  0.0  1.0  0.0  0.0
                0.0  0.0  0.0  0.0  0.0
                0.0  0.0  0.0  0.0  0.0
                0.0  0.0  0.0  0.0  0.0]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L406-L436">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.assemble-Tuple{Union{PyCall.PyObject, #s191} where #s191&lt;:Integer,Union{PyCall.PyObject, #s190} where #s190&lt;:Integer,PyCall.PyObject}" href="#ADCME.assemble-Tuple{Union{PyCall.PyObject, #s191} where #s191&lt;:Integer,Union{PyCall.PyObject, #s190} where #s190&lt;:Integer,PyCall.PyObject}"><code>ADCME.assemble</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">assemble(m::Union{PyObject, &lt;:Integer}, n::Union{PyObject, &lt;:Integer}, ops::PyObject)</code></pre><p>Assembles the sparse matrix from the <code>ops</code> created by <a href="#Base.accumulate"><code>accumulate</code></a>. <code>ops</code> is either a single output from <code>accumulate</code>, or concated from several <code>ops</code></p><pre><code class="language-julia">op1 = accumulate(handle, 1, [1;2;3], [1.0;2.0;3.0])
op2 = accumulate(handle, 2, [1;2;3], [1.0;2.0;3.0])
op = [op1;op2] # equivalent to `vcat([op1, op2]...)`</code></pre><p><code>m</code> and <code>n</code> are rows and columns of the sparse matrix. </p><p>See <a href="#ADCME.SparseAssembler"><code>SparseAssembler</code></a> for an example.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L473-L485">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.find-Tuple{SparseTensor}" href="#ADCME.find-Tuple{SparseTensor}"><code>ADCME.find</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">find(s::SparseTensor)</code></pre><p>Returns the row, column and values for sparse tensor <code>s</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L76-L80">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.scatter_add-Union{Tuple{T}, Tuple{S}, Tuple{Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}},Union{Colon, UnitRange{T}, Array{S,1}, Integer, PyCall.PyObject},Union{Colon, UnitRange{T}, Array{T,1}, Integer, PyCall.PyObject},Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}}}} where T&lt;:Real where S&lt;:Real" href="#ADCME.scatter_add-Union{Tuple{T}, Tuple{S}, Tuple{Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}},Union{Colon, UnitRange{T}, Array{S,1}, Integer, PyCall.PyObject},Union{Colon, UnitRange{T}, Array{T,1}, Integer, PyCall.PyObject},Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}}}} where T&lt;:Real where S&lt;:Real"><code>ADCME.scatter_add</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">scatter_update(A::Union{SparseTensor, SparseMatrixCSC{Float64,Int64}},
i1::Union{Integer, Colon, UnitRange{T}, PyObject,Array{S,1}},
i2::Union{Integer, Colon, UnitRange{T}, PyObject,Array{T,1}},
B::Union{SparseTensor, SparseMatrixCSC{Float64,Int64}})  where {S&lt;:Real,T&lt;:Real}</code></pre><p>Adds <code>B</code> to a subblock of a sparse matrix <code>A</code>. Equivalently, </p><pre><code class="language-none">A[i1, i2] += B</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L325-L335">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.scatter_update-Union{Tuple{T}, Tuple{S}, Tuple{Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}},Union{Colon, UnitRange{S}, Array{S,1}, Integer, PyCall.PyObject},Union{Colon, UnitRange{T}, Array{T,1}, Integer, PyCall.PyObject},Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}}}} where T&lt;:Real where S&lt;:Real" href="#ADCME.scatter_update-Union{Tuple{T}, Tuple{S}, Tuple{Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}},Union{Colon, UnitRange{S}, Array{S,1}, Integer, PyCall.PyObject},Union{Colon, UnitRange{T}, Array{T,1}, Integer, PyCall.PyObject},Union{SparseTensor, SparseArrays.SparseMatrixCSC{Float64,Int64}}}} where T&lt;:Real where S&lt;:Real"><code>ADCME.scatter_update</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">scatter_update(A::Union{SparseTensor, SparseMatrixCSC{Float64,Int64}},
i1::Union{Integer, Colon, UnitRange{T}, PyObject,Array{S,1}},
i2::Union{Integer, Colon, UnitRange{T}, PyObject,Array{T,1}},
B::Union{SparseTensor, SparseMatrixCSC{Float64,Int64}})  where {S&lt;:Real,T&lt;:Real}</code></pre><p>Updates a subblock of a sparse matrix by <code>B</code>. Equivalently, </p><pre><code class="language-none">A[i1, i2] = B</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L288-L298">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.solve-Tuple{Tuple{SparseTensor,PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject}}" href="#ADCME.solve-Tuple{Tuple{SparseTensor,PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject}}"><code>ADCME.solve</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">solve(A_factorized::Tuple{SparseTensor, PyObject}, rhs::Union{Array{Float64,1}, PyObject})</code></pre><p>Solves the equation <code>A_factorized * x = rhs</code> using the factorized sparse matrix. See <a href="#LinearAlgebra.factorize"><code>factorize</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L643-L647">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.spdiag-Tuple{Int64}" href="#ADCME.spdiag-Tuple{Int64}"><code>ADCME.spdiag</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">spdiag(n::Int64)</code></pre><p>Constructs a sparse identity matrix of size <span>$n\times n$</span>, which is equivalent to <code>spdiag(n, 0=&gt;ones(n))</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L499-L503">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.spdiag-Tuple{Integer,Vararg{Pair,N} where N}" href="#ADCME.spdiag-Tuple{Integer,Vararg{Pair,N} where N}"><code>ADCME.spdiag</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">spdiag(m::Integer, pair::Pair...)</code></pre><p>Constructs a square <span>$m\times m$</span> <a href="#ADCME.SparseTensor"><code>SparseTensor</code></a> from pairs of the form </p><pre><code class="language-none">offset =&gt; array </code></pre><p><strong>Example</strong></p><p>Suppose we want to construct a <span>$10\times 10$</span> tridiagonal matrix, where the lower off-diagonals are all -2,  the diagonals are all 2, and the upper off-diagonals are all 3, the corresponding Julia code is </p><pre><code class="language-julia">spdiag(10, -1=&gt;-2*ones(9), 0=&gt;2*ones(10), 1=&gt;3ones(9))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L575-L588">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.spdiag-Tuple{PyCall.PyObject}" href="#ADCME.spdiag-Tuple{PyCall.PyObject}"><code>ADCME.spdiag</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">spdiag(o::PyObject)</code></pre><p>Constructs a sparse diagonal matrix where the diagonal entries are <code>o</code>, which is equivalent to <code>spdiag(length(o), 0=&gt;o)</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L508-L512">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.spzero" href="#ADCME.spzero"><code>ADCME.spzero</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">spzero(m::Int64, n::Union{Missing, Int64}=missing)</code></pre><p>Constructs a empty sparse matrix of size <span>$m\times n$</span>. <code>n=m</code> if <code>n</code> is <code>missing</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L521-L525">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.accumulate-Tuple{PyCall.PyObject,Union{PyCall.PyObject, #s191} where #s191&lt;:Integer,Union{PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Integer},Union{PyCall.PyObject, Array{#s189,N} where N where #s189&lt;:Real}}" href="#Base.accumulate-Tuple{PyCall.PyObject,Union{PyCall.PyObject, #s191} where #s191&lt;:Integer,Union{PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Integer},Union{PyCall.PyObject, Array{#s189,N} where N where #s189&lt;:Real}}"><code>Base.accumulate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">accumulate(handle::PyObject, row::Union{PyObject, &lt;:Integer}, cols::Union{PyObject, Array{&lt;:Integer}}, vals::Union{PyObject, Array{&lt;:Real}})</code></pre><p>Accumulates <code>row</code>-th row. It adds the value to the sparse matrix</p><pre><code class="language-julia">for k = 1:length(cols)
    A[row, cols[k]] += vals[k]
end</code></pre><p><code>handle</code> is the handle created by <a href="#ADCME.SparseAssembler"><code>SparseAssembler</code></a>. </p><p>See <a href="#ADCME.SparseAssembler"><code>SparseAssembler</code></a> for an example.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The function <code>accumulate</code> returns a <code>op::PyObject</code>. Only when <code>op</code> is executed, the nonzero values are populated into the sparse matrix. </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L447-L462">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LinearAlgebra.factorize" href="#LinearAlgebra.factorize"><code>LinearAlgebra.factorize</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">factorize(A::Union{SparseTensor, SparseMatrixCSC}, max_cache_size::Int64 = 999999)</code></pre><p>Factorizes <span>$A$</span> for sparse matrix solutions. <code>max_cache_size</code> specifies the maximum cache sizes in the C++ kernels,  which determines the maximum number of factorized matrices.  The function returns the factorized matrix, which is basically <code>Tuple{SparseTensor, PyObject}</code>. </p><p><strong>Example</strong></p><pre><code class="language-julia">A = sprand(10,10,0.7)
Afac = factorize(A) # factorizing the matrix
run(sess, Afac\rand(10)) # no factorization, solving the equation
run(sess, Afac\rand(10)) # no factorization, solving the equation</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L615-L628">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:\\" href="#Base.:\\"><code>Base.:\</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">\(A::SparseTensor, o::PyObject, method::String=&quot;SparseLU&quot;)</code></pre><p>Solves the linear equation  <span>$A x = o$</span></p><p><strong>Method</strong></p><p>For square matrices <span>$A$</span>, one of the following methods is available</p><ul><li><code>auto</code>: using the solver specified by <code>ADCME.options.sparse.solver</code></li><li><code>SparseLU</code></li><li><code>SparseQR</code></li><li><code>SimplicialLDLT</code></li><li><code>SimplicialLLT</code></li></ul><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>In the case <code>o</code> is 2 dimensional, <code>\</code> is understood as &quot;batched solve&quot;. <code>o</code> must have size <span>$n_{b} \times m$</span>, and  <span>$A$</span> has a size <span>$m\times n$</span>. It returns the solution matrix of size <span>$n_b \times n$</span></p><div>\[s_{i,:} = A^{-1} o_{i,:}\]</div></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L350-L369">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:\\-Tuple{Tuple{SparseTensor,PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject}}" href="#Base.:\\-Tuple{Tuple{SparseTensor,PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject}}"><code>Base.:\</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Base.:\(A_factorized::Tuple{SparseTensor, PyObject}, rhs::Union{Array{Float64,1}, PyObject})</code></pre><p>A convenient overload for <a href="#ADCME.solve-Tuple{Tuple{SparseTensor,PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject}}"><code>solve</code></a>. See <a href="#LinearAlgebra.factorize"><code>factorize</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/sparse.jl#L660-L664">source</a></section></article><h2 id="Operations-1"><a class="docs-heading-anchor" href="#Operations-1">Operations</a><a class="docs-heading-anchor-permalink" href="#Operations-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.argsort-Tuple{PyCall.PyObject}" href="#ADCME.argsort-Tuple{PyCall.PyObject}"><code>ADCME.argsort</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">argsort(o::PyObject; 
stable::Bool = false, rev::Bool=false, dims::Integer=-1, name::Union{Nothing,String}=nothing)</code></pre><p>Returns the indices of a tensor that give its sorted order along an axis.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L1059-L1064">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.batch_matmul-Tuple{PyCall.PyObject,PyCall.PyObject}" href="#ADCME.batch_matmul-Tuple{PyCall.PyObject,PyCall.PyObject}"><code>ADCME.batch_matmul</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">batch_matmul(o1::PyObject, o2::PyObject)</code></pre><p>Computes <code>o1[i,:,:] * o2[i, :]</code> or <code>o1[i,:,:] * o2[i, :, :]</code> for each index <code>i</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L76-L80">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.clip-Tuple{Union{Array{Any,N} where N, Array{PyCall.PyObject,N} where N},Any,Any,Vararg{Any,N} where N}" href="#ADCME.clip-Tuple{Union{Array{Any,N} where N, Array{PyCall.PyObject,N} where N},Any,Any,Vararg{Any,N} where N}"><code>ADCME.clip</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">clip(o::Union{Array{Any}, Array{PyObject}}, vmin, vmax, args...;kwargs...)</code></pre><p>Clips the values of <code>o</code> to the range [<code>vmin</code>, <code>vmax</code>]</p><p><strong>Example</strong></p><pre><code class="language-julia">a = constant(3.0)
a = clip(a, 1.0, 2.0)
b = constant(rand(3))
b = clip(b, 0.5, 1.0)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L786-L798">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.cvec-Tuple{PyCall.PyObject}" href="#ADCME.cvec-Tuple{PyCall.PyObject}"><code>ADCME.cvec</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">rvec(o::PyObject; kwargs...)</code></pre><p>Vectorizes the tensor <code>o</code> to a column vector, assuming column major.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L223-L227">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.pad-Tuple{Union{PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Real},Array{Int64,2},Vararg{Any,N} where N}" href="#ADCME.pad-Tuple{Union{PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Real},Array{Int64,2},Vararg{Any,N} where N}"><code>ADCME.pad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">pad(o::PyObject, paddings::Array{Int64, 2}, args...; kwargs...)</code></pre><p>Pads <code>o</code> with values on the boundary. </p><p><strong>Example</strong></p><pre><code class="language-julia">o = rand(3,3)
o = pad(o, [1 4      # first dimension
             2 3])   # second dimension
run(sess, o)</code></pre><p>Expected:</p><pre><code class="language-none">8×8 Array{Float64,2}:
 0.0  0.0  0.0       0.0       0.0       0.0  0.0  0.0
 0.0  0.0  0.250457  0.666905  0.823611  0.0  0.0  0.0
 0.0  0.0  0.23456   0.625145  0.646713  0.0  0.0  0.0
 0.0  0.0  0.552415  0.226417  0.67802   0.0  0.0  0.0
 0.0  0.0  0.0       0.0       0.0       0.0  0.0  0.0
 0.0  0.0  0.0       0.0       0.0       0.0  0.0  0.0
 0.0  0.0  0.0       0.0       0.0       0.0  0.0  0.0
 0.0  0.0  0.0       0.0       0.0       0.0  0.0  0.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L428-L451">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.pmap-Tuple{Function,Union{PyCall.PyObject, Array{PyCall.PyObject,N} where N}}" href="#ADCME.pmap-Tuple{Function,Union{PyCall.PyObject, Array{PyCall.PyObject,N} where N}}"><code>ADCME.pmap</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">pmap(fn::Function, o::Union{Array{PyObject}, PyObject})</code></pre><p>Parallel for loop. There should be no data dependency between different iterations.</p><p><strong>Example</strong></p><pre><code class="language-julia">x = constant(ones(10))
y1 = pmap(x-&gt;2.0*x, x)
y2 = pmap(x-&gt;x[1]+x[2], [x,x])
y3 = pmap(1:10, x) do z
    i = z[1]
    xi = z[2]
    xi + cast(Float64, i)
end
run(sess, y1)
run(sess, y2)
run(sess, y3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L907-L926">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.rvec-Tuple{PyCall.PyObject}" href="#ADCME.rvec-Tuple{PyCall.PyObject}"><code>ADCME.rvec</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">rvec(o::PyObject; kwargs...)</code></pre><p>Vectorizes the tensor <code>o</code> to a row vector, assuming column major.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L205-L209">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.scatter_add-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}" href="#ADCME.scatter_add-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}"><code>ADCME.scatter_add</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">scatter_add(A::PyObject, 
    xind::Union{Colon, Int64, Array{Int64}, BitArray{1}, Array{Bool,1}, UnitRange{Int64}, StepRange{Int64, Int64}, PyObject},
    yind::Union{Colon, Int64, Array{Int64}, BitArray{1}, Array{Bool,1}, UnitRange{Int64}, StepRange{Int64, Int64}, PyObject},
    updates::Union{Array{&lt;:Real}, Real, PyObject})</code></pre><pre><code class="language-julia">A[xind, yind] += updates</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L732-L741">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.scatter_add-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}" href="#ADCME.scatter_add-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}"><code>ADCME.scatter_add</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">scatter_add(a::PyObject, 
    indices::Union{Colon, Int64, Array{Int64}, BitArray{1}, Array{Bool,1}, UnitRange{Int64}, StepRange{Int64, Int64}, PyObject},
    updates::Union{Array{&lt;:Real}, Real, PyObject})</code></pre><p>Updates array <code>ref</code></p><pre><code class="language-none">a[indices] += updates</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L680-L689">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.scatter_sub-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}" href="#ADCME.scatter_sub-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}"><code>ADCME.scatter_sub</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">scatter_add(A::PyObject, 
    xind::Union{Colon, Int64, Array{Int64}, BitArray{1}, Array{Bool,1}, UnitRange{Int64}, StepRange{Int64, Int64}, PyObject},
    yind::Union{Colon, Int64, Array{Int64}, BitArray{1}, Array{Bool,1}, UnitRange{Int64}, StepRange{Int64, Int64}, PyObject},
    updates::Union{Array{&lt;:Real}, Real, PyObject})</code></pre><pre><code class="language-julia">A[xind, yind] -= updates</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L747-L756">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.scatter_sub-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}" href="#ADCME.scatter_sub-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}"><code>ADCME.scatter_sub</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">scatter_sub(a::PyObject, 
    indices::Union{Colon, Int64, Array{Int64}, BitArray{1}, Array{Bool,1}, UnitRange{Int64}, StepRange{Int64, Int64}, PyObject},
    updates::Union{Array{&lt;:Real}, Real, PyObject})</code></pre><p>Updates array <code>ref</code></p><pre><code class="language-none">a[indices] -= updates</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L665-L674">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.scatter_update-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}" href="#ADCME.scatter_update-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}"><code>ADCME.scatter_update</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">scatter_update(A::PyObject, 
    xind::Union{Colon, Int64, Array{Int64}, BitArray{1}, Array{Bool,1}, UnitRange{Int64}, StepRange{Int64, Int64}, PyObject},
    yind::Union{Colon, Int64, Array{Int64}, BitArray{1}, Array{Bool,1}, UnitRange{Int64}, StepRange{Int64, Int64}, PyObject},
    updates::Union{Array{&lt;:Real}, Real, PyObject})</code></pre><pre><code class="language-julia">A[xind, yind] = updates</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L717-L726">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.scatter_update-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}" href="#ADCME.scatter_update-Tuple{PyCall.PyObject,Union{Colon, StepRange{Int64,Int64}, UnitRange{Int64}, Int64, BitArray{1}, Array{Bool,1}, PyCall.PyObject, Array{Int64,N} where N},Union{Real, PyCall.PyObject, Array{#s217,N} where N where #s217&lt;:Real}}"><code>ADCME.scatter_update</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">scatter_update(a::PyObject, 
    indices::Union{Colon, Int64, Array{Int64}, BitArray{1}, Array{Bool,1}, UnitRange{Int64}, StepRange{Int64, Int64}, PyObject},
    updates::Union{Array{&lt;:Real}, Real, PyObject})</code></pre><p>Updates array <code>ref</code></p><pre><code class="language-none">a[indices] = updates</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L651-L660">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.set_shape-Union{Tuple{N}, Tuple{PyCall.PyObject,Union{Tuple{Vararg{Integer,N}}, Array{#s192,N} where N where #s192&lt;:Integer}}} where N" href="#ADCME.set_shape-Union{Tuple{N}, Tuple{PyCall.PyObject,Union{Tuple{Vararg{Integer,N}}, Array{#s192,N} where N where #s192&lt;:Integer}}} where N"><code>ADCME.set_shape</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">set_shape(o::PyObject, s::Union{Array{&lt;:Integer}, Tuple{Vararg{&lt;:Integer, N}}}) where N
set_shape(o::PyObject, s::Integer...)</code></pre><p>Sets the shape of <code>o</code> to <code>s</code>. <code>s</code> must be the actual shape of <code>o</code>. This function is used to convert a  tensor with unknown dimensions to a tensor with concrete dimensions. </p><p><strong>Example</strong></p><pre><code class="language-julia">a = placeholder(Float64, shape=[nothing, 10])
b = set_shape(a, 3, 10)
run(sess, b, a=&gt;rand(3,10)) # OK 
run(sess, b, a=&gt;rand(5,10)) # Error
run(sess, b, a=&gt;rand(10,3)) # Error</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L257-L272">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.solve_batch-Tuple{Union{PyCall.PyObject, Array{#s191,2} where #s191&lt;:Real},Union{PyCall.PyObject, Array{#s190,2} where #s190&lt;:Real}}" href="#ADCME.solve_batch-Tuple{Union{PyCall.PyObject, Array{#s191,2} where #s191&lt;:Real},Union{PyCall.PyObject, Array{#s190,2} where #s190&lt;:Real}}"><code>ADCME.solve_batch</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">solve_batch(A::Union{PyObject, Array{&lt;:Real, 2}}, rhs::Union{PyObject, Array{&lt;:Real,2}})</code></pre><p>Solves <span>$Ax = b$</span> for a batch of right hand sides. </p><ul><li><code>A</code>: a <span>$m\times n$</span> matrix, where <span>$m\geq n$</span></li><li><code>rhs</code>: a <span>$n_b\times m$</span> matrix. Each row is a new right hand side to solve. </li></ul><p>The returned value is a <span>$n_b\times n$</span> matrix. </p><p><strong>Example</strong></p><pre><code class="language-julia">a = rand(10,5)
b = rand(100, 10)
sol = solve_batch(a, b)
@assert run(sess, sol) ≈ (a\b&#39;)&#39;</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Internally, the matrix <span>$A$</span> is factorized first and then the factorization is used to solve multiple right hand side.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L498-L518">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.stack-Tuple{PyCall.PyObject}" href="#ADCME.stack-Tuple{PyCall.PyObject}"><code>ADCME.stack</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">stack(o::PyObject)</code></pre><p>Convert a <code>TensorArray</code> <code>o</code> to a normal tensor. The leading dimension is the size of the tensor array. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L596-L600">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.topk" href="#ADCME.topk"><code>ADCME.topk</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">topk(o::PyObject, k::Union{PyObject,Integer}=1;
    sorted::Bool=true, name::Union{Nothing,String}=nothing)</code></pre><p>Finds values and indices of the <code>k</code> largest entries for the last dimension. If <code>sorted=true</code> the resulting k elements will be sorted by the values in descending order.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L1046-L1052">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.vector-Union{Tuple{T}, Tuple{Union{PyCall.PyObject, StepRange, UnitRange, Array{T,N} where N},Union{PyCall.PyObject, Array{Float64,N} where N},Union{Int64, PyCall.PyObject}}} where T&lt;:Integer" href="#ADCME.vector-Union{Tuple{T}, Tuple{Union{PyCall.PyObject, StepRange, UnitRange, Array{T,N} where N},Union{PyCall.PyObject, Array{Float64,N} where N},Union{Int64, PyCall.PyObject}}} where T&lt;:Integer"><code>ADCME.vector</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">vector(i::Union{Array{T}, PyObject, UnitRange, StepRange}, v::Union{Array{Float64},PyObject},s::Union{Int64,PyObject})</code></pre><p>Returns a vector <code>V</code> with length <code>s</code> such that</p><pre><code class="language-none">V[i] = v</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L979-L986">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.adjoint-Tuple{PyCall.PyObject}" href="#Base.adjoint-Tuple{PyCall.PyObject}"><code>Base.adjoint</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">adjoint(o::PyObject; kwargs...)</code></pre><p>Returns the conjugate adjoint of <code>o</code>.  When the dimension of <code>o</code> is greater than 2, only the last two dimensions are permuted, i.e., <code>permutedims(o, [1,2,...,n,n-1])</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L478-L483">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.vec-Tuple{PyCall.PyObject}" href="#Base.vec-Tuple{PyCall.PyObject}"><code>Base.vec</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">vec(o::PyObject;kwargs...)</code></pre><p>Vectorizes the tensor <code>o</code> assuming column major. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L241-L245">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LinearAlgebra.svd-Tuple{PyCall.PyObject,Vararg{Any,N} where N}" href="#LinearAlgebra.svd-Tuple{PyCall.PyObject,Vararg{Any,N} where N}"><code>LinearAlgebra.svd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">svd(o::PyObject, args...; kwargs...)</code></pre><p>Returns a <code>TFSVD</code> structure which holds the following data structures</p><pre><code class="language-julia">S::PyObject
U::PyObject
V::PyObject
Vt::PyObject</code></pre><p>We have the equality <span>$o = USV&#39;$</span></p><p><strong>Example</strong></p><pre><code class="language-julia">A = rand(10,20)
r = svd(constant(A))
A2 = r.U*diagm(r.S)*r.Vt # The value of `A2` should be equal to `A`</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L828-L847">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.map-Tuple{Function,Union{PyCall.PyObject, Array{PyCall.PyObject,N} where N}}" href="#Base.map-Tuple{Function,Union{PyCall.PyObject, Array{PyCall.PyObject,N} where N}}"><code>Base.map</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">map(fn::Function, o::Union{Array{PyObject},PyObject};
kwargs...)</code></pre><p>Applies <code>fn</code> to each element of <code>o</code>. </p><ul><li><code>o</code>∈<code>Array{PyObject}</code> : returns <code>[fn(x) for x in o]</code></li><li><code>o</code>∈PyObject : splits <code>o</code> according to the first dimension and then applies <code>fn</code>. </li></ul><p><strong>Example</strong></p><pre><code class="language-julia">a = constant(rand(10,5))
b = map(x-&gt;sum(x), a) # equivalent to `sum(a, dims=2)`</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If <code>fn</code> is a multivariate function, we need to specify the output type using <code>dtype</code> keyword. For example, </p><pre><code class="language-julia">a = constant(ones(10))
b = constant(ones(10))
fn = x-&gt;x[1]+x[2]
c = map(fn, [a, b], dtype=Float64)</code></pre></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L873-L895">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.reshape-Union{Tuple{N}, Tuple{PyCall.PyObject,Union{Tuple{Vararg{Integer,N}}, Array{#s192,N} where N where #s192&lt;:Integer}}} where N" href="#Base.reshape-Union{Tuple{N}, Tuple{PyCall.PyObject,Union{Tuple{Vararg{Integer,N}}, Array{#s192,N} where N where #s192&lt;:Integer}}} where N"><code>Base.reshape</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">reshape(o::PyObject, s::Union{Array{&lt;:Integer}, Tuple{Vararg{&lt;:Integer, N}}}) where N 
reshape(o::PyObject, s::Integer; kwargs...)
reshape(o::PyObject, m::Integer, n::Integer; kwargs...)
reshape(o::PyObject, ::Colon, n::Integer)
reshape(o::PyObject, n::Integer, ::Colon)</code></pre><p>Reshapes the tensor according to row major if the &quot;TensorFlow style&quot; syntax is used; otherwise  reshaping according to column major is assumed. </p><p><strong>Example</strong></p><pre><code class="language-julia">reshape(a, [10,5]) # row major 
reshape(a, 10, 5) # column major </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L166-L181">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.reverse-Tuple{PyCall.PyObject}" href="#Base.reverse-Tuple{PyCall.PyObject}"><code>Base.reverse</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">reverse(o::PyObject, kwargs...)</code></pre><p>Given a tensor <code>o</code>, and an index <code>dims</code> representing the set of dimensions of tensor to reverse.</p><p><strong>Example</strong></p><pre><code class="language-julia">a = rand(10,2)
A = constant(a)
@assert run(sess, reverse(A, dims=1)) == reverse(a, dims=1)
@assert run(sess, reverse(A, dims=2)) == reverse(a, dims=2)
@assert run(sess, reverse(A, dims=-1)) == reverse(a, dims=2)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L1172-L1185">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.sort-Tuple{PyCall.PyObject}" href="#Base.sort-Tuple{PyCall.PyObject}"><code>Base.sort</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Base.:sort(o::PyObject; 
rev::Bool=false, dims::Integer=-1, name::Union{Nothing,String}=nothing)</code></pre><p>Sort a multidimensional array <code>o</code> along the given dimension. </p><ul><li><code>rev</code>: <code>true</code> for DESCENDING and <code>false</code> (default) for ASCENDING</li><li><code>dims</code>: <code>-1</code> for last dimension. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L1028-L1035">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.split-Tuple{PyCall.PyObject,Union{Integer, PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Integer}}" href="#Base.split-Tuple{PyCall.PyObject,Union{Integer, PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Integer}}"><code>Base.split</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">split(o::PyObject, 
    num_or_size_splits::Union{Integer, Array{&lt;:Integer}, PyObject}; kwargs...)</code></pre><p>Splits <code>o</code> according to <code>num_or_size_splits</code></p><p><strong>Example 1</strong></p><pre><code class="language-julia">a = constant(rand(10,8,6))
split(a, 5)</code></pre><p>Expected output:</p><pre><code class="language-none">5-element Array{PyCall.PyObject,1}:
 PyObject &lt;tf.Tensor &#39;split_5:0&#39; shape=(2, 8, 6) dtype=float64&gt;
 PyObject &lt;tf.Tensor &#39;split_5:1&#39; shape=(2, 8, 6) dtype=float64&gt;
 PyObject &lt;tf.Tensor &#39;split_5:2&#39; shape=(2, 8, 6) dtype=float64&gt;
 PyObject &lt;tf.Tensor &#39;split_5:3&#39; shape=(2, 8, 6) dtype=float64&gt;
 PyObject &lt;tf.Tensor &#39;split_5:4&#39; shape=(2, 8, 6) dtype=float64&gt;</code></pre><p><strong>Example 2</strong></p><pre><code class="language-julia">a = constant(rand(10,8,6))
split(a, [4,3,1], dims=2)</code></pre><p>Expected output:</p><pre><code class="language-none">3-element Array{PyCall.PyObject,1}:
 PyObject &lt;tf.Tensor &#39;split_6:0&#39; shape=(10, 4, 6) dtype=float64&gt;
 PyObject &lt;tf.Tensor &#39;split_6:1&#39; shape=(10, 3, 6) dtype=float64&gt;
 PyObject &lt;tf.Tensor &#39;split_6:2&#39; shape=(10, 1, 6) dtype=float64&gt;</code></pre><p><strong>Example 3</strong></p><pre><code class="language-julia">a = constant(rand(10,8,6))
split(a, 3, dims=3)</code></pre><p>Expected output:</p><pre><code class="language-none">3-element Array{PyCall.PyObject,1}:
 PyObject &lt;tf.Tensor &#39;split_7:0&#39; shape=(10, 8, 2) dtype=float64&gt;
 PyObject &lt;tf.Tensor &#39;split_7:1&#39; shape=(10, 8, 2) dtype=float64&gt;
 PyObject &lt;tf.Tensor &#39;split_7:2&#39; shape=(10, 8, 2) dtype=float64&gt;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ops.jl#L1119-L1165">source</a></section></article><h2 id="IO-1"><a class="docs-heading-anchor" href="#IO-1">IO</a><a class="docs-heading-anchor-permalink" href="#IO-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.Diary" href="#ADCME.Diary"><code>ADCME.Diary</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Diary(suffix::Union{String, Nothing}=nothing)</code></pre><p>Creates a diary at a temporary directory path. It returns a writer and the corresponding directory path</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/io.jl#L142-L146">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.activate" href="#ADCME.activate"><code>ADCME.activate</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">activate(sw::Diary, port::Int64=6006)</code></pre><p>Running <a href="#ADCME.Diary"><code>Diary</code></a> at http://localhost:port.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/io.jl#L172-L176">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.load" href="#ADCME.load"><code>ADCME.load</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">load(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)</code></pre><p>Loads the values of variables to the session <code>sess</code> from the file <code>file</code>. If <code>vars</code> is nothing, it loads values to all the trainable variables. See also <a href="#ADCME.save"><code>save</code></a>, <a href="#ADCME.load"><code>load</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/io.jl#L88-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.load-Tuple{Diary,String}" href="#ADCME.load-Tuple{Diary,String}"><code>ADCME.load</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">load(sw::Diary, dirp::String)</code></pre><p>Loads <a href="#ADCME.Diary"><code>Diary</code></a> from <code>dirp</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/io.jl#L162-L166">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.logging-Tuple{Union{Nothing, String},Vararg{PyCall.PyObject,N} where N}" href="#ADCME.logging-Tuple{Union{Nothing, String},Vararg{PyCall.PyObject,N} where N}"><code>ADCME.logging</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">logging(file::Union{Nothing,String}, o::PyObject...; summarize::Int64 = 3, sep::String = &quot; &quot;)</code></pre><p>Logging <code>o</code> to <code>file</code>. This operator must be used with <a href="#Base.bind-Tuple{PyCall.PyObject,Vararg{Any,N} where N}"><code>bind</code></a>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/io.jl#L216-L220">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.pload-Tuple{String}" href="#ADCME.pload-Tuple{String}"><code>ADCME.pload</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">pload(file::String)</code></pre><p>Loads a Python objection from <code>file</code>. See also <a href="#ADCME.psave-Tuple{PyCall.PyObject,String}"><code>psave</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/io.jl#L32-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.psave-Tuple{PyCall.PyObject,String}" href="#ADCME.psave-Tuple{PyCall.PyObject,String}"><code>ADCME.psave</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">psave(o::PyObject, file::String)</code></pre><p>Saves a Python objection <code>o</code> to <code>file</code>. See also <a href="#ADCME.pload-Tuple{String}"><code>pload</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/io.jl#L19-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.save" href="#ADCME.save"><code>ADCME.save</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">save(sess::PyObject, file::String, vars::Union{PyObject, Nothing, Array{PyObject}}=nothing, args...; kwargs...)</code></pre><p>Saves the values of <code>vars</code> in the session <code>sess</code>. The result is written into <code>file</code> as a dictionary. If <code>vars</code> is nothing, it saves all the trainable variables. See also <a href="#ADCME.save"><code>save</code></a>, <a href="#ADCME.load"><code>load</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/io.jl#L47-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.save-Tuple{Diary,String}" href="#ADCME.save-Tuple{Diary,String}"><code>ADCME.save</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">save(sw::Diary, dirp::String)</code></pre><p>Saves <a href="#ADCME.Diary"><code>Diary</code></a> to <code>dirp</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/io.jl#L153-L157">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.scalar" href="#ADCME.scalar"><code>ADCME.scalar</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">scalar(o::PyObject, name::String)</code></pre><p>Returns a scalar summary object.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/io.jl#L182-L186">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.write-Tuple{Diary,Int64,Union{String, Array{String,N} where N}}" href="#Base.write-Tuple{Diary,Int64,Union{String, Array{String,N} where N}}"><code>Base.write</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">write(sw::Diary, step::Int64, cnt::Union{String, Array{String}})</code></pre><p>Writes to <a href="#ADCME.Diary"><code>Diary</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/io.jl#L194-L198">source</a></section></article><h2 id="Optimization-1"><a class="docs-heading-anchor" href="#Optimization-1">Optimization</a><a class="docs-heading-anchor-permalink" href="#Optimization-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.BFGS!" href="#ADCME.BFGS!"><code>ADCME.BFGS!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">BFGS!(value_and_gradients_function::Function, initial_position::Union{PyObject, Array{Float64}}, max_iter::Int64=50, args...;kwargs...)</code></pre><p>Applies the BFGS optimizer to <code>value_and_gradients_function</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/optim.jl#L259-L263">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.BFGS!" href="#ADCME.BFGS!"><code>ADCME.BFGS!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">BFGS!(sess::PyObject, loss::PyObject, max_iter::Int64=15000; 
vars::Array{PyObject}=PyObject[], callback::Union{Function, Nothing}=nothing, kwargs...)</code></pre><p><code>BFGS!</code> is a simplified interface for BFGS optimizer. See also <a href="#ADCME.ScipyOptimizerInterface-Tuple{Any}"><code>ScipyOptimizerInterface</code></a>. <code>callback</code> is a callback function with signature </p><pre><code class="language-julia">callback(vs::Array, iter::Int64, loss::Float64)</code></pre><p><code>vars</code> is an array consisting of tensors and its values will be the input to <code>vs</code>.</p><p><strong>Example 1</strong></p><pre><code class="language-julia">a = Variable(1.0)
loss = (a - 10.0)^2
sess = Session(); init(sess)
BFGS!(sess, loss)</code></pre><p><strong>Example 2</strong></p><pre><code class="language-julia">θ1 = Variable(1.0)
θ2 = Variable(1.0)
loss = (θ1-1)^2 + (θ2-2)^2
cb = (vs, iter, loss)-&gt;begin 
    printstyled(&quot;[#iter $iter] θ1=$(vs[1]), θ2=$(vs[2]), loss=$loss\n&quot;, color=:green)
end
sess = Session(); init(sess)
cb(run(sess, [θ1, θ2]), 0, run(sess, loss))
BFGS!(sess, loss, 100; vars=[θ1, θ2], callback=cb)</code></pre><p><strong>Example 3</strong></p><p>Use <code>bounds</code> to specify upper and lower bound of a variable. </p><pre><code class="language-julia">x = Variable(2.0)    
loss = x^2
sess = Session(); init(sess)
BFGS!(sess, loss, bounds=Dict(x=&gt;[1.0,3.0]))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/optim.jl#L171-L211">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.BFGS!-Union{Tuple{T}, Tuple{PyCall.PyObject,PyCall.PyObject,Union{Nothing, PyCall.PyObject, Array{T,N} where N},Union{PyCall.PyObject, Array{PyCall.PyObject,N} where N}}} where T&lt;:Union{Nothing, PyCall.PyObject}" href="#ADCME.BFGS!-Union{Tuple{T}, Tuple{PyCall.PyObject,PyCall.PyObject,Union{Nothing, PyCall.PyObject, Array{T,N} where N},Union{PyCall.PyObject, Array{PyCall.PyObject,N} where N}}} where T&lt;:Union{Nothing, PyCall.PyObject}"><code>ADCME.BFGS!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">BFGS!(sess::PyObject, loss::PyObject, grads::Union{Array{T},Nothing,PyObject}, 
vars::Union{Array{PyObject},PyObject}; kwargs...) where T&lt;:Union{Nothing, PyObject}</code></pre><p>Running BFGS algorithm <span>$\min_{\texttt{vars}} \texttt{loss}(\texttt{vars})$</span> The gradients <code>grads</code> must be provided. Typically, <code>grads[i] = gradients(loss, vars[i])</code>.  <code>grads[i]</code> can exist on different devices (GPU or CPU). </p><p><strong>Example 1</strong></p><pre><code class="language-julia">import Optim # required
a = Variable(0.0)
loss = (a-1)^2
g = gradients(loss, a)
sess = Session(); init(sess)
BFGS!(sess, loss, g, a)</code></pre><p><strong>Example 2</strong></p><pre><code class="language-julia">import Optim # required
a = Variable(0.0)
loss = (a^2+a-1)^2
g = gradients(loss, a)
sess = Session(); init(sess)
cb = (vs, iter, loss)-&gt;begin 
    printstyled(&quot;[#iter $iter] a = $vs, loss=$loss\n&quot;, color=:green)
end
BFGS!(sess, loss, g, a; callback = cb)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/optim.jl#L602-L633">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.CustomOptimizer-Tuple{Function}" href="#ADCME.CustomOptimizer-Tuple{Function}"><code>ADCME.CustomOptimizer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">CustomOptimizer(opt::Function, name::String)</code></pre><p>creates a custom optimizer with struct name <code>name</code>. For example, we can integrate <code>Optim.jl</code> with <code>ADCME</code> by  constructing a new optimizer</p><pre><code class="language-julia">CustomOptimizer(&quot;Con&quot;) do f, df, c, dc, x0, x_L, x_U
    opt = Opt(:LD_MMA, length(x0))
    bd = zeros(length(x0)); bd[end-1:end] = [-Inf, 0.0]
    opt.lower_bounds = bd
    opt.xtol_rel = 1e-4
    opt.min_objective = (x,g)-&gt;(g[:]= df(x); return f(x)[1])
    inequality_constraint!(opt, (x,g)-&gt;( g[:]= dc(x);c(x)[1]), 1e-8)
    (minf,minx,ret) = NLopt.optimize(opt, x0)
    minx
end</code></pre><p>Here</p><p>∘ <code>f</code>: a function that returns <span>$f(x)$</span></p><p>∘ <code>df</code>: a function that returns <span>$\nabla f(x)$</span></p><p>∘ <code>c</code>: a function that returns the constraints <span>$c(x)$</span></p><p>∘ <code>dc</code>: a function that returns <span>$\nabla c(x)$</span></p><p>∘ <code>x0</code>: initial guess</p><p>∘ <code>nineq</code>: number of inequality constraints</p><p>∘ <code>neq</code>: number of equality constraints</p><p>∘ <code>x_L</code>: lower bounds of optimizable variables</p><p>∘ <code>x_U</code>: upper bounds of optimizable variables</p><p>Then we can create an optimizer with </p><pre><code class="language-none">opt = Con(loss, inequalities=[c1], equalities=[c2])</code></pre><p>To trigger the optimization, use</p><pre><code class="language-none">minimize(opt, sess)</code></pre><p>Note thanks to the global variable scope of Julia, <code>step_callback</code>, <code>optimizer_kwargs</code> can actually  be passed from Julia environment directly.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/optim.jl#L72-L120">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.NonlinearConstrainedProblem-Union{Tuple{T}, Tuple{Function,Function,Union{Array{Float64,1}, PyCall.PyObject},Union{PyCall.PyObject, Array{Float64,N} where N}}} where T&lt;:Real" href="#ADCME.NonlinearConstrainedProblem-Union{Tuple{T}, Tuple{Function,Function,Union{Array{Float64,1}, PyCall.PyObject},Union{PyCall.PyObject, Array{Float64,N} where N}}} where T&lt;:Real"><code>ADCME.NonlinearConstrainedProblem</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">NonlinearConstrainedProblem(f::Function, L::Function, θ::PyObject, u0::Union{PyObject, Array{Float64}}; options::Union{Dict{String, T}, Missing}=missing) where T&lt;:Integer</code></pre><p>Computes the gradients <span>$\frac{\partial L}{\partial \theta}$</span></p><div>\[\min \ L(u) \quad \mathrm{s.t.} \ F(\theta, u) = 0\]</div><p><code>u0</code> is the initial guess for the numerical solution <code>u</code>, see <a href="../newton_raphson/#ADCME.newton_raphson"><code>newton_raphson</code></a>.</p><p>Caveats: Assume <code>r, A = f(θ, u)</code> and <code>θ</code> are the unknown parameters, <code>gradients(r, θ)</code> must be defined (backprop works properly)</p><p>Returns: It returns a tuple (<code>L</code>: loss, <code>C</code>: constraints, and <code>Graidents</code>)</p><div>\[\left(L(u), u, \frac{\partial L}{\partial θ}\right)\]</div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/optim.jl#L568-L587">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.ScipyOptimizerInterface-Tuple{Any}" href="#ADCME.ScipyOptimizerInterface-Tuple{Any}"><code>ADCME.ScipyOptimizerInterface</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ScipyOptimizerInterface(loss; method=&quot;L-BFGS-B&quot;, options=Dict(&quot;maxiter&quot;=&gt; 15000, &quot;ftol&quot;=&gt;1e-12, &quot;gtol&quot;=&gt;1e-12), kwargs...)</code></pre><p>A simple interface for Scipy Optimizer. See also <a href="#ADCME.ScipyOptimizerMinimize-Tuple{PyCall.PyObject,PyCall.PyObject}"><code>ScipyOptimizerMinimize</code></a> and <a href="#ADCME.BFGS!"><code>BFGS!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/optim.jl#L45-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.ScipyOptimizerMinimize-Tuple{PyCall.PyObject,PyCall.PyObject}" href="#ADCME.ScipyOptimizerMinimize-Tuple{PyCall.PyObject,PyCall.PyObject}"><code>ADCME.ScipyOptimizerMinimize</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ScipyOptimizerMinimize(sess::PyObject, opt::PyObject; kwargs...)</code></pre><p>Minimizes a scalar Tensor. Variables subject to optimization are updated in-place at the end of optimization.</p><p>Note that this method does not just return a minimization Op, unlike <code>minimize</code>; instead it actually performs minimization by executing commands to control a Session https://www.tensorflow.org/api_docs/python/tf/contrib/opt/ScipyOptimizerInterface. See also <a href="#ADCME.ScipyOptimizerInterface-Tuple{Any}"><code>ScipyOptimizerInterface</code></a> and <a href="#ADCME.BFGS!"><code>BFGS!</code></a>.</p><ul><li>feed_dict: A feed dict to be passed to calls to session.run.</li><li>fetches: A list of Tensors to fetch and supply to loss_callback as positional arguments.</li><li>step_callback: A function to be called at each optimization step; arguments are the current values of all optimization variables flattened into a single vector.</li><li>loss_callback: A function to be called every time the loss and gradients are computed, with evaluated fetches supplied as positional arguments.</li><li>run_kwargs: kwargs to pass to session.run.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/optim.jl#L53-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.newton_raphson-Union{Tuple{T}, Tuple{Function,Union{PyCall.PyObject, Array}}, Tuple{Function,Union{PyCall.PyObject, Array},Union{Missing, PyCall.PyObject, Array{#s192,N} where N where #s192&lt;:Real},Vararg{PyCall.PyObject,N} where N}} where T&lt;:Real" href="#ADCME.newton_raphson-Union{Tuple{T}, Tuple{Function,Union{PyCall.PyObject, Array}}, Tuple{Function,Union{PyCall.PyObject, Array},Union{Missing, PyCall.PyObject, Array{#s192,N} where N where #s192&lt;:Real},Vararg{PyCall.PyObject,N} where N}} where T&lt;:Real"><code>ADCME.newton_raphson</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">newton_raphson(func::Function, 
    u0::Union{Array,PyObject}, 
    θ::Union{Missing,PyObject, Array{&lt;:Real}}=missing,
    args::PyObject...) where T&lt;:Real</code></pre><p>Newton Raphson solver for solving a nonlinear equation.  ∘ <code>func</code> has the signature </p><ul><li><code>func(θ::Union{Missing,PyObject}, u::PyObject)-&gt;(r::PyObject, A::Union{PyObject,SparseTensor})</code> (if <code>linesearch</code> is off)</li><li><code>func(θ::Union{Missing,PyObject}, u::PyObject)-&gt;(fval::PyObject, r::PyObject, A::Union{PyObject,SparseTensor})</code> (if <code>linesearch</code> is on)</li></ul><p>where <code>r</code> is the residual and <code>A</code> is the Jacobian matrix; in the case where <code>linesearch</code> is on, the function value <code>fval</code> must also be supplied. ∘ <code>θ</code> are external parameters. ∘ <code>u0</code> is the initial guess for <code>u</code> ∘ <code>args</code>: additional inputs to the func function  ∘ <code>kwargs</code>: keyword arguments to <code>func</code></p><p>The solution can be configured via <code>ADCME.options.newton_raphson</code></p><ul><li><code>max_iter</code>: maximum number of iterations (default=100)</li><li><code>rtol</code>: relative tolerance for termination (default=1e-12)</li><li><code>tol</code>: absolute tolerance for termination (default=1e-12)</li><li><code>LM</code>: a float number, Levenberg-Marquardt modification <span>$x^{k+1} = x^k - (J^k + \mu^k)^{-1}g^k$</span> (default=0.0)</li><li><code>linesearch</code>: whether linesearch is used (default=false)</li></ul><p>Currently, the backtracing algorithm is implemented. The parameters for <code>linesearch</code> are supplied via <code>options.newton_raphson.linesearch_options</code></p><ul><li><code>c1</code>: stop criterion, <span>$f(x^k) &lt; f(0) + \alpha c_1  f&#39;(0)$</span></li><li><code>ρ_hi</code>: the new step size <span>$\alpha_1\leq \rho_{hi}\alpha_0$</span> </li><li><code>ρ_lo</code>: the new step size <span>$\alpha_1\geq \rho_{lo}\alpha_0$</span> </li><li><code>iterations</code>: maximum number of iterations for linesearch</li><li><code>maxstep</code>: maximum allowable steps</li><li><code>αinitial</code>: initial guess for the step size <span>$\alpha$</span></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/optim.jl#L347-L380">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.newton_raphson_with_grad-Union{Tuple{T}, Tuple{Function,Union{PyCall.PyObject, Array}}, Tuple{Function,Union{PyCall.PyObject, Array},Union{Missing, PyCall.PyObject, Array{#s112,N} where N where #s112&lt;:Real},Vararg{PyCall.PyObject,N} where N}} where T&lt;:Real" href="#ADCME.newton_raphson_with_grad-Union{Tuple{T}, Tuple{Function,Union{PyCall.PyObject, Array}}, Tuple{Function,Union{PyCall.PyObject, Array},Union{Missing, PyCall.PyObject, Array{#s112,N} where N where #s112&lt;:Real},Vararg{PyCall.PyObject,N} where N}} where T&lt;:Real"><code>ADCME.newton_raphson_with_grad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">newton_raphson_with_grad(f::Function, 
u0::Union{Array,PyObject}, 
θ::Union{Missing,PyObject, Array{&lt;:Real}}=missing,
args::PyObject...) where T&lt;:Real</code></pre><p>Differentiable Newton-Raphson algorithm. See <a href="../newton_raphson/#ADCME.newton_raphson"><code>newton_raphson</code></a>.</p><p>Use <code>ADCME.options.newton_raphson</code> to supply options. </p><p><strong>Example</strong></p><pre><code class="language-julia">function f(θ, x)
    x^3 - θ, 3spdiag(x^2)
end

θ = constant([2. .^3;3. ^3; 4. ^3])
x = newton_raphson_with_grad(f, constant(ones(3)), θ)
run(sess, x)≈[2.;3.;4.]
run(sess, gradients(sum(x), θ))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/optim.jl#L513-L534">source</a></section></article><h2 id="Neural-Networks-1"><a class="docs-heading-anchor" href="#Neural-Networks-1">Neural Networks</a><a class="docs-heading-anchor-permalink" href="#Neural-Networks-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.BatchNormalization" href="#ADCME.BatchNormalization"><code>ADCME.BatchNormalization</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BatchNormalization(dims::Int64=2; kwargs...)</code></pre><p>Creates a batch normalization layer. </p><p><strong>Example</strong></p><pre><code class="language-julia">b = BatchNormalization(2)
x = rand(10,2)
training = placeholder(true)
y = b(x, training)
run(sess, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L519-L532">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.Conv1D" href="#ADCME.Conv1D"><code>ADCME.Conv1D</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Conv1D(filters, kernel_size, strides, activation, args...;kwargs...)</code></pre><pre><code class="language-julia">c = Conv1D(32, 3, 1, &quot;relu&quot;)
x = rand(100, 6, 128) # 128-length vectors with 6 timesteps (&quot;channels&quot;)
y = c(x) # shape=(100, 4, 32)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L593-L601">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.Conv2D" href="#ADCME.Conv2D"><code>ADCME.Conv2D</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Conv2D(filters, kernel_size, strides, activation, args...;kwargs...)</code></pre><p>The arrangement is (samples, rows, cols, channels) (data<em>format=&#39;channels</em>last&#39;)</p><pre><code class="language-julia">Conv2D(32, 3, 1, &quot;relu&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L624-L631">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.Conv3D" href="#ADCME.Conv3D"><code>ADCME.Conv3D</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Conv3D(filters, kernel_size, strides, activation, args...;kwargs...)</code></pre><p>The arrangement is (samples, rows, cols, channels) (data<em>format=&#39;channels</em>last&#39;)</p><pre><code class="language-julia">c = Conv3D(32, 3, 1, &quot;relu&quot;)
x = constant(rand(100, 10, 10, 10, 16))
y = c(x)
# shape=(100, 8, 8, 8, 32)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L675-L685">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.Dense" href="#ADCME.Dense"><code>ADCME.Dense</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Dense(units::Int64, activation::Union{String, Function, Nothing} = nothing,
    args...;kwargs...)</code></pre><p>Creates a callable dense neural network.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L567-L572">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.Resnet1D" href="#ADCME.Resnet1D"><code>ADCME.Resnet1D</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Resnet1D(out_features::Int64, hidden_features::Int64;
    num_blocks::Int64=2, activation::Union{String, Function, Nothing} = &quot;relu&quot;, 
    dropout_probability::Float64 = 0.0, use_batch_norm::Bool = false, name::Union{String, Missing} = missing)</code></pre><p>Creates a 1D residual network. If <code>name</code> is not missing, <code>Resnet1D</code> does not create a new entity. </p><p><strong>Example</strong></p><pre><code class="language-julia">resnet = Resnet1D(20)
x = rand(1000,10)
y = resnet(x)</code></pre><p><strong>Example: Digit recognition</strong></p><pre><code class="language-none">using MLDatasets
using ADCME

# load data 
train_x, train_y = MNIST.traindata()
train_x = reshape(Float64.(train_x), :, size(train_x,3))&#39;|&gt;Array
test_x, test_y = MNIST.testdata()
test_x = reshape(Float64.(test_x), :, size(test_x,3))&#39;|&gt;Array

# construct loss function 
ADCME.options.training.training = placeholder(true)
x = placeholder(rand(64, 784))
l = placeholder(rand(Int64, 64))
resnet = Resnet1D(10, num_blocks=10)
y = resnet(x)
loss = mean(sparse_softmax_cross_entropy_with_logits(labels=l, logits=y))

# train the neural network 
opt = AdamOptimizer().minimize(loss)
sess = Session(); init(sess)
for i = 1:10000
    idx = rand(1:60000, 64)
    _, loss_ = run(sess, [opt, loss], feed_dict=Dict(l=&gt;train_y[idx], x=&gt;train_x[idx,:]))
    @info i, loss_
end

# test 
for i = 1:10
    idx = rand(1:10000,100)
    y0 = resnet(test_x[idx,:])
    y0 = run(sess, y0, ADCME.options.training.training=&gt;false)
    pred = [x[2]-1 for x in argmax(y0, dims=2)]
    @info &quot;Accuracy = &quot;, sum(pred .== test_y[idx])/100
end</code></pre><p><img src="https://github.com/ADCMEMarket/ADCMEImages/tree/master/ADCME/assets/resnet.png?raw=true" alt/></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L749-L800">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.ae" href="#ADCME.ae"><code>ADCME.ae</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ae(x::PyObject, output_dims::Array{Int64}, scope::String = &quot;default&quot;;
    activation::Union{Function,String} = &quot;tanh&quot;)</code></pre><p>Alias: <code>fc</code></p><p>Creates a neural network with intermediate numbers of neurons <code>output_dims</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L89-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.ae-Tuple{Union{PyCall.PyObject, Array{Float64,N} where N},Array{Int64,N} where N,Union{Array{Array{Float64,N} where N,N} where N, Array{PyCall.PyObject,N} where N}}" href="#ADCME.ae-Tuple{Union{PyCall.PyObject, Array{Float64,N} where N},Array{Int64,N} where N,Union{Array{Array{Float64,N} where N,N} where N, Array{PyCall.PyObject,N} where N}}"><code>ADCME.ae</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ae(x::Union{Array{Float64}, PyObject}, 
    output_dims::Array{Int64}, 
    θ::Union{Array{Array{Float64}}, Array{PyObject}};
    activation::Union{Function,String} = &quot;tanh&quot;)</code></pre><p>Alias: <code>fc</code></p><p>Constructs a neural network with given weights and biases <code>θ</code></p><p><strong>Example</strong></p><pre><code class="language-julia">x = constant(rand(10,30))
θ = ae_init([30, 20, 20, 5])
y = ae(x, [20, 20, 5], θ) # 10×5</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L180-L196">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.ae-Tuple{Union{PyCall.PyObject, Array{Float64,N} where N},Array{Int64,N} where N,Union{PyCall.PyObject, Array{Float64,N} where N}}" href="#ADCME.ae-Tuple{Union{PyCall.PyObject, Array{Float64,N} where N},Array{Int64,N} where N,Union{PyCall.PyObject, Array{Float64,N} where N}}"><code>ADCME.ae</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ae(x::Union{Array{Float64}, PyObject}, output_dims::Array{Int64}, θ::Union{Array{Float64}, PyObject};
activation::Union{Function,String, Nothing} = &quot;tanh&quot;)</code></pre><p>Alias: <code>fc</code></p><p>Creates a neural network with intermediate numbers of neurons <code>output_dims</code>. The weights are given by <code>θ</code></p><p><strong>Example 1: Explicitly construct weights and biases</strong></p><pre><code class="language-julia">x = constant(rand(10,2))
n = ae_num([2,20,20,20,2])
θ = Variable(randn(n)*0.001)
y = ae(x, [20,20,20,2], θ)</code></pre><p><strong>Example 2: Implicitly construct weights and biases</strong></p><pre><code class="language-julia">θ = ae_init([10,20,20,20,2]) 
x = constant(rand(10,10))
y = ae(x, [20,20,20,2], θ)</code></pre><p>See also <a href="#ADCME.ae_num-Tuple{Array{Int64,N} where N}"><code>ae_num</code></a>, <a href="#ADCME.ae_init-Tuple{Array{Int64,N} where N}"><code>ae_init</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L120-L144">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.ae_init-Tuple{Array{Int64,N} where N}" href="#ADCME.ae_init-Tuple{Array{Int64,N} where N}"><code>ADCME.ae_init</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ae_init(output_dims::Array{Int64}; T::Type=Float64, method::String=&quot;xavier&quot;)
fc_init(output_dims::Array{Int64})</code></pre><p>Return the initial weights and bias values by TensorFlow as a vector. The neural network architecture is</p><div>\[o_1 (Input layer) \rightarrow o_2 \rightarrow \cdots \rightarrow o_n (Output layer)\]</div><p>Three types of  random initializers are provided</p><ul><li><code>xavier</code> (default). It is useful for <code>tanh</code> fully connected neural network. </li></ul><div>\[W^l_i \sim \sqrt{\frac{1}{n_{l-1}}}\]</div><ul><li><code>xavier_avg</code>. A variant of <code>xavier</code></li></ul><div>\[W^l_i \sim \sqrt{\frac{2}{n_l + n_{l-1}}}\]</div><ul><li><code>he</code>. This is the activation aware initialization of weights and helps mitigate the problem</li></ul><p>of vanishing/exploding gradients. </p><div>\[W^l_i \sim \sqrt{\frac{2}{n_{l-1}}}\]</div><p><strong>Example</strong></p><pre><code class="language-julia">x = constant(rand(10,30))
θ = ae_init([30, 20, 20, 5])
y = ae(x, [20, 20, 5], θ) # 10×5</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L218-L249">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.ae_num-Tuple{Array{Int64,N} where N}" href="#ADCME.ae_num-Tuple{Array{Int64,N} where N}"><code>ADCME.ae_num</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ae_num(output_dims::Array{Int64})
fc_num(output_dims::Array{Int64})</code></pre><p>Estimates the number of weights and biases for the neural network. Note the first dimension should be the feature dimension (this is different from <a href="#ADCME.ae"><code>ae</code></a> since in <code>ae</code> the feature dimension can be inferred), and the last dimension should be the output dimension. </p><p><strong>Example</strong></p><pre><code class="language-julia">x = constant(rand(10,30))
θ = ae_init([30, 20, 20, 5])
@assert ae_num([30, 20, 20, 5])==length(θ)
y = ae(x, [20, 20, 5], θ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L273-L288">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.ae_to_code-Tuple{String,String}" href="#ADCME.ae_to_code-Tuple{String,String}"><code>ADCME.ae_to_code</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ae_to_code(file::String, scope::String; activation::String = &quot;tanh&quot;)</code></pre><p>Return the code string from the feed-forward neural network data in <code>file</code>. Usually we can immediately evaluate  the code string into Julia session by </p><pre><code class="language-julia">eval(Meta.parse(s))</code></pre><p>If <code>activation</code> is not specified, <code>tanh</code> is the default. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L326-L335">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.bn-Tuple" href="#ADCME.bn-Tuple"><code>ADCME.bn</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">bn(args...;center = true, scale=true, kwargs...)</code></pre><p><code>bn</code> accepts a keyword parameter <code>is_training</code>. </p><p><strong>Example</strong></p><pre><code class="language-julia">bn(inputs, name=&quot;batch_norm&quot;, is_training=true)</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p><code>bn</code> should be used with <code>control_dependency</code></p><pre><code class="language-julia">update_ops = get_collection(UPDATE_OPS)
control_dependencies(update_ops) do 
    global train_step = AdamOptimizer().minimize(loss)
end </code></pre></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L379-L396">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.dense-Tuple{Union{PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Real},Int64,Vararg{Any,N} where N}" href="#ADCME.dense-Tuple{Union{PyCall.PyObject, Array{#s190,N} where N where #s190&lt;:Real},Int64,Vararg{Any,N} where N}"><code>ADCME.dense</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">dense(inputs::Union{PyObject, Array{&lt;:Real}}, units::Int64, args...; 
    activation::Union{String, Function} = nothing, kwargs...)</code></pre><p>Creates a fully connected layer with the activation function specified by <code>activation</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L367-L372">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.dropout" href="#ADCME.dropout"><code>ADCME.dropout</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">dropout(x::Union{PyObject, Real, Array{&lt;:Real}}, 
rate::Union{Real, PyObject}, training::Union{PyObject,Bool} = true; kwargs...)</code></pre><p>Randomly drops out entries in <code>x</code> with a rate of <code>rate</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L495-L500">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.fc" href="#ADCME.fc"><code>ADCME.fc</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ae(x::PyObject, output_dims::Array{Int64}, scope::String = &quot;default&quot;;
    activation::Union{Function,String} = &quot;tanh&quot;)</code></pre><p>Alias: <code>fc</code></p><p>Creates a neural network with intermediate numbers of neurons <code>output_dims</code>.</p><pre><code class="language-none">ae(x::Union{Array{Float64}, PyObject}, output_dims::Array{Int64}, θ::Union{Array{Float64}, PyObject};
activation::Union{Function,String, Nothing} = &quot;tanh&quot;)</code></pre><p>Alias: <code>fc</code></p><p>Creates a neural network with intermediate numbers of neurons <code>output_dims</code>. The weights are given by <code>θ</code></p><p><strong>Example 1: Explicitly construct weights and biases</strong></p><pre><code class="language-julia">x = constant(rand(10,2))
n = ae_num([2,20,20,20,2])
θ = Variable(randn(n)*0.001)
y = ae(x, [20,20,20,2], θ)</code></pre><p><strong>Example 2: Implicitly construct weights and biases</strong></p><pre><code class="language-julia">θ = ae_init([10,20,20,20,2]) 
x = constant(rand(10,10))
y = ae(x, [20,20,20,2], θ)</code></pre><p>See also <a href="#ADCME.ae_num-Tuple{Array{Int64,N} where N}"><code>ae_num</code></a>, <a href="#ADCME.ae_init-Tuple{Array{Int64,N} where N}"><code>ae_init</code></a>.</p><pre><code class="language-none">ae(x::Union{Array{Float64}, PyObject}, 
    output_dims::Array{Int64}, 
    θ::Union{Array{Array{Float64}}, Array{PyObject}};
    activation::Union{Function,String} = &quot;tanh&quot;)</code></pre><p>Alias: <code>fc</code></p><p>Constructs a neural network with given weights and biases <code>θ</code></p><p><strong>Example</strong></p><pre><code class="language-julia">x = constant(rand(10,30))
θ = ae_init([30, 20, 20, 5])
y = ae(x, [20, 20, 5], θ) # 10×5</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L478">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.fc_init" href="#ADCME.fc_init"><code>ADCME.fc_init</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ae_init(output_dims::Array{Int64}; T::Type=Float64, method::String=&quot;xavier&quot;)
fc_init(output_dims::Array{Int64})</code></pre><p>Return the initial weights and bias values by TensorFlow as a vector. The neural network architecture is</p>$<p>o<em>1 (Input layer) \rightarrow o</em>2 \rightarrow \cdots \rightarrow o_n (Output layer) $</p><p>Three types of  random initializers are provided</p><ul><li><code>xavier</code> (default). It is useful for <code>tanh</code> fully connected neural network.</li></ul>$<p>W^l<em>i \sim \sqrt{\frac{1}{n</em>{l-1}}} $</p><ul><li><code>xavier_avg</code>. A variant of <code>xavier</code></li></ul>$<p>W^l<em>i \sim \sqrt{\frac{2}{n</em>l + n_{l-1}}} $</p><ul><li><code>he</code>. This is the activation aware initialization of weights and helps mitigate the problem</li></ul><p>of vanishing/exploding gradients. </p>$<p>W^l<em>i \sim \sqrt{\frac{2}{n</em>{l-1}}} $</p><p><strong>Example</strong></p><pre><code class="language-julia">x = constant(rand(10,30))
θ = ae_init([30, 20, 20, 5])
y = ae(x, [20, 20, 5], θ) # 10×5</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L486">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.fc_num" href="#ADCME.fc_num"><code>ADCME.fc_num</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ae_num(output_dims::Array{Int64})
fc_num(output_dims::Array{Int64})</code></pre><p>Estimates the number of weights and biases for the neural network. Note the first dimension should be the feature dimension (this is different from <a href="#ADCME.ae"><code>ae</code></a> since in <code>ae</code> the feature dimension can be inferred), and the last dimension should be the output dimension. </p><p><strong>Example</strong></p><pre><code class="language-julia">x = constant(rand(10,30))
θ = ae_init([30, 20, 20, 5])
@assert ae_num([30, 20, 20, 5])==length(θ)
y = ae(x, [20, 20, 5], θ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L482">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.fcx-Tuple{Union{Array{Float64,2}, PyCall.PyObject},Array{Int64,1},Union{Array{Float64,1}, PyCall.PyObject}}" href="#ADCME.fcx-Tuple{Union{Array{Float64,2}, PyCall.PyObject},Array{Int64,1},Union{Array{Float64,1}, PyCall.PyObject}}"><code>ADCME.fcx</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">fcx(x::Union{Array{Float64,2},PyObject}, output_dims::Array{Int64,1}, 
θ::Union{Array{Float64,1}, PyObject};
activation::String = &quot;tanh&quot;)</code></pre><p>Creates a fully connected neural network with output dimension <code>o</code> and inputs <span>$x\in \mathbb{R}^{m\times n}$</span>. </p><div>\[x \rightarrow o_1 \rightarrow o_2 \rightarrow \ldots \rightarrow o_k\]</div><p><code>θ</code> is the weights and biases of the neural network, e.g., <code>θ = ae_init(output_dims)</code>.</p><p><code>fcx</code> outputs two tensors:</p><ul><li><p>the output of the neural network: <span>$u\in \mathbb{R}^{m\times o_k}$</span>.</p></li><li><p>the sensitivity of the neural network per sample: <span>$\frac{\partial u}{\partial x}\in \mathbb{R}^{m \times o_k \times n}$</span></p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/layers.jl#L56-L73">source</a></section></article><h2 id="Generative-Neural-Nets-1"><a class="docs-heading-anchor" href="#Generative-Neural-Nets-1">Generative Neural Nets</a><a class="docs-heading-anchor-permalink" href="#Generative-Neural-Nets-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.GAN" href="#ADCME.GAN"><code>ADCME.GAN</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">GAN(dat::Union{Array,PyObject}, generator::Function, discriminator::Function,
loss::Union{Missing, Function}=missing; latent_dim::Union{Missing, Int64}=missing,
    batch_size::Int64=32)</code></pre><p>Creates a GAN instance. </p><ul><li><code>dat</code> <span>$\in \mathbb{R}^{n\times d}$</span> is the training data for the GAN, where <span>$n$</span> is the number of training data, and <span>$d$</span> is the dimension per training data.</li><li><code>generator</code><span>$:\mathbb{R}^{d&#39;} \rightarrow \mathbb{R}^d$</span> is the generator function, <span>$d&#39;$</span> is the hidden dimension.</li><li><code>discriminator</code><span>$:\mathbb{R}^{d} \rightarrow \mathbb{R}$</span> is the discriminator function. </li><li><code>loss</code> is the loss function. See <a href="#ADCME.klgan-Tuple{GAN}"><code>klgan</code></a>, <a href="#ADCME.rklgan-Tuple{GAN}"><code>rklgan</code></a>, <a href="#ADCME.wgan-Tuple{GAN}"><code>wgan</code></a>, <a href="#ADCME.lsgan-Tuple{GAN}"><code>lsgan</code></a> for examples.</li><li><code>latent_dim</code> (default=<span>$d$</span>, the same as output dimension) is the latent dimension.</li><li><code>batch_size</code> (default=32) is the batch size in training.</li></ul><p><strong>Example: Constructing a GAN</strong></p><pre><code class="language-julia">dat = rand(10000,10)
generator = (z, gan)-&gt;10*z
discriminator = (x, gan)-&gt;sum(x)
gan = GAN(dat, generator, discriminator, &quot;wgan_stable&quot;)</code></pre><p><strong>Example: Learning a Gaussian random variable</strong></p><pre><code class="language-julia">using ADCME 
using PyPlot
using Distributions
dat = randn(10000, 1) * 0.5 .+ 3.0
function gen(z, gan)
    ae(z, [20,20,20,1], &quot;generator_$(gan.ganid)&quot;, activation = &quot;relu&quot;)
end
function disc(x, gan)
    squeeze(ae(x, [20,20,20,1], &quot;discriminator_$(gan.ganid)&quot;, activation = &quot;relu&quot;))
end
gan = GAN(dat, gen, disc, g-&gt;wgan_stable(g, 0.001); latent_dim = 10)

dopt = AdamOptimizer(0.0002, beta1=0.5, beta2=0.9).minimize(gan.d_loss, var_list=gan.d_vars)
gopt = AdamOptimizer(0.0002, beta1=0.5, beta2=0.9).minimize(gan.g_loss, var_list=gan.g_vars)
sess = Session(); init(sess)
for i = 1:5000
    batch_x = rand(1:10000, 32)
    batch_z = randn(32, 10)
    for n_critic = 1:1
        global _, dl = run(sess, [dopt, gan.d_loss], 
                feed_dict=Dict(gan.ids=&gt;batch_x, gan.noise=&gt;batch_z))
    end
    _, gl, gm, dm, gp = run(sess, [gopt, gan.g_loss, 
        gan.STORAGE[&quot;g_grad_magnitude&quot;], gan.STORAGE[&quot;d_grad_magnitude&quot;], 
        gan.STORAGE[&quot;gradient_penalty&quot;]],
        feed_dict=Dict(gan.ids=&gt;batch_x, gan.noise=&gt;batch_z))
    mod(i, 100)==0 &amp;&amp; (@info i, dl, gl, gm, dm, gp)
end

hist(run(sess, squeeze(rand(gan,10000))), bins=50, density = true)
nm = Normal(3.0,0.5)
x0 = 1.0:0.01:5.0
y0 = pdf.(nm, x0)
plot(x0, y0, &quot;g&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/gan.jl#L61-L120">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.jsgan-Tuple{GAN}" href="#ADCME.jsgan-Tuple{GAN}"><code>ADCME.jsgan</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">jsgan(gan::GAN)</code></pre><p>Computes the vanilla GAN loss function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/gan.jl#L199-L203">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.klgan-Tuple{GAN}" href="#ADCME.klgan-Tuple{GAN}"><code>ADCME.klgan</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">klgan(gan::GAN)</code></pre><p>Computes the KL-divergence GAN loss function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/gan.jl#L185-L189">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.lsgan-Tuple{GAN}" href="#ADCME.lsgan-Tuple{GAN}"><code>ADCME.lsgan</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">lsgan(gan::GAN)</code></pre><p>Computes the least square GAN loss function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/gan.jl#L279-L283">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.predict-Tuple{GAN,Union{PyCall.PyObject, Array}}" href="#ADCME.predict-Tuple{GAN,Union{PyCall.PyObject, Array}}"><code>ADCME.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">predict(gan::GAN, input::Union{PyObject, Array})</code></pre><p>Predicts the GAN <code>gan</code> output given input <code>input</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/gan.jl#L313-L317">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.rklgan-Tuple{GAN}" href="#ADCME.rklgan-Tuple{GAN}"><code>ADCME.rklgan</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">rklgan(gan::GAN)</code></pre><p>Computes the reverse KL-divergence GAN loss function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/gan.jl#L265-L269">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.sample-Tuple{GAN,Int64}" href="#ADCME.sample-Tuple{GAN,Int64}"><code>ADCME.sample</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">sample(gan::GAN, n::Int64)
rand(gan::GAN, n::Int64)</code></pre><p>Samples <code>n</code> instances from <code>gan</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/gan.jl#L294-L299">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.wgan-Tuple{GAN}" href="#ADCME.wgan-Tuple{GAN}"><code>ADCME.wgan</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">wgan(gan::GAN)</code></pre><p>Computes the Wasserstein GAN loss function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/gan.jl#L213-L217">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.wgan_stable" href="#ADCME.wgan_stable"><code>ADCME.wgan_stable</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">wgan_stable(gan::GAN, λ::Float64)</code></pre><p>Returns the discriminator and generator loss for the Wasserstein GAN loss with penalty parameter <span>$\lambda$</span></p><p>The objective function is </p><div>\[L = E_{\tilde x\sim P_g} [D(\tilde x)] - E_{x\sim P_r} [D(x)] + \lambda E_{\hat x\sim P_{\hat x}}[(||\nabla_{\hat x}D(\hat x)||^2-1)^2]\]</div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/gan.jl#L227-L236">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.build!-Tuple{GAN}" href="#ADCME.build!-Tuple{GAN}"><code>ADCME.build!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">build!(gan::GAN)</code></pre><p>Builds the GAN instances. This function returns <code>gan</code> for convenience.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/gan.jl#L35-L39">source</a></section></article><h2 id="Tools-1"><a class="docs-heading-anchor" href="#Tools-1">Tools</a><a class="docs-heading-anchor-permalink" href="#Tools-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.clean-Tuple{}" href="#ADCME.clean-Tuple{}"><code>ADCME.clean</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">clean()</code></pre><p>Clean up CustomOps directory. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L181-L185">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.customop" href="#ADCME.customop"><code>ADCME.customop</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">customop(simple::Bool=false)</code></pre><p>Create a new custom operator. If <code>simple=true</code>, the custom operator only supports CPU and does not have gradients. </p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; customop() # create an editable `customop.txt` file
[ Info: Edit custom_op.txt for custom operators
julia&gt; customop() # after editing `customop.txt`, call it again to generate interface files.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L354-L366">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.debug-Tuple{PyCall.PyObject,PyCall.PyObject}" href="#ADCME.debug-Tuple{PyCall.PyObject,PyCall.PyObject}"><code>ADCME.debug</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">debug(sess::PyObject, o::PyObject)</code></pre><p>In the case a session run yields an error from the TensorFlow backend, this function can help print the exact error.  For example, you might encounter  <code>InvalidArgumentError()</code> with no detailed error information, and this function can be useful for debugging.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L626-L631">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.doctor-Tuple{}" href="#ADCME.doctor-Tuple{}"><code>ADCME.doctor</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">doctor()</code></pre><p>Reports health of the current installed ADCME package. If some components are broken, possible fix is proposed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L643-L647">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.install-Tuple{String}" href="#ADCME.install-Tuple{String}"><code>ADCME.install</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">install(s::String; force::Bool = false)</code></pre><p>Install a custom operator via URL. <code>s</code> can be</p><ul><li>A URL. ADCME will download the directory through <code>git</code></li><li>A string. ADCME will search for the associated package on https://github.com/ADCMEMarket</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L432-L438">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.install_adept" href="#ADCME.install_adept"><code>ADCME.install_adept</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">install_adept(force::Bool=false)</code></pre><p>Install adept-2 library: https://github.com/rjhogan/Adept-2</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L533-L537">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.load_op-Tuple{String,String}" href="#ADCME.load_op-Tuple{String,String}"><code>ADCME.load_op</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">load_op(oplibpath::String, opname::String)</code></pre><p>Loads the operator <code>opname</code> from library <code>oplibpath</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L76-L80">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.load_op_and_grad-Tuple{String,String}" href="#ADCME.load_op_and_grad-Tuple{String,String}"><code>ADCME.load_op_and_grad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">load_op_and_grad(oplibpath::String, opname::String; multiple::Bool=false)</code></pre><p>Loads the operator <code>opname</code> from library <code>oplibpath</code>; gradients are also imported.  If <code>multiple</code> is true, the operator is assumed to have multiple outputs. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L118-L123">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.load_system_op" href="#ADCME.load_system_op"><code>ADCME.load_system_op</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">load_system_op(s::String, oplib::String, grad::Bool=true)</code></pre><p>Loads custom operator from CustomOps directory (shipped with ADCME instead of TensorFlow) For example </p><pre><code class="language-none">s = &quot;SparseOperator&quot;
oplib = &quot;libSO&quot;
grad = true</code></pre><p>this will direct Julia to find library <code>CustomOps/SparseOperator/libSO.dylib</code> on MACOSX</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L231-L242">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.nnuq-Tuple{Array{Float64,2},Union{Float64, Array{Float64,2}},Union{Float64, Array{Float64,2}}}" href="#ADCME.nnuq-Tuple{Array{Float64,2},Union{Float64, Array{Float64,2}},Union{Float64, Array{Float64,2}}}"><code>ADCME.nnuq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">nnuq(H::Array{Float64,2}, invR::Union{Float64, Array{Float64,2}}, invQ::Union{Float64, Array{Float64,2}})</code></pre><p>Returns the variance matrix for the Baysian inversion. </p><p>The negative log likelihood function is</p><div>\[l(s) =\frac{1}{2} (y-h(s))^T R^{-1} (y-h(s)) + \frac{1}{2} s^T Q^{-1} s\]</div><p>The covariance matrix is computed by first linearizing <span>$h(s)$</span></p><div>\[h(s)\approx h(s_0) + \nabla h(s_0) (s-s_0)\]</div><p>and then computing the second order derivative</p><div>\[V = \left(\frac{\partial^2 l}{\partial s^T\partial s}\right)^{-1} = (H^T R^{-1} H + Q^{-1})^{-1}\]</div><p>Note the result is independent of <span>$s_0$</span>, <span>$y_0$</span>, and only depends on <span>$\nabla h(s_0)$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L886-L904">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.register-Tuple{Function,Function}" href="#ADCME.register-Tuple{Function,Function}"><code>ADCME.register</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">register(forward::Function, backward::Function; multiple::Bool=false)</code></pre><p>Register a function <code>forward</code> with back-propagated gradients rule <code>backward</code> to the backward.  ∘ <code>forward</code>: it takes <span>$n$</span> inputs and outputs <span>$m$</span> tensors. When <span>$m&gt;1$</span>, the keyword <code>multiple</code> must be true.  ∘ <code>backward</code>: it takes <span>$\tilde m$</span> top gradients from float/double output tensors of <code>forward</code>, <span>$m$</span> outputs of the <code>forward</code>,     and <span>$n$</span> inputs of the <code>forward</code>. <code>backward</code> outputs <span>$n$</span> gradients for each input of <code>forward</code>. When input <span>$i$</span> of    <code>forward</code> is not float/double, <code>backward</code> should return <code>nothing</code> for the corresponding gradients. </p><p><strong>Example</strong></p><pre><code class="language-julia">forward = x-&gt;log(1+exp(x))
backward = (dy, y, x)-&gt;dy*(1-1/(1+y))
f = register(forward, backward)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L583-L598">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.test_jacobian-Tuple{Function,Array{Float64,N} where N}" href="#ADCME.test_jacobian-Tuple{Function,Array{Float64,N} where N}"><code>ADCME.test_jacobian</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">test_jacobian(f::Function, x0::Array{Float64}; scale::Float64 = 1.0)</code></pre><p>Testing the gradients of a vector function <code>f</code>: <code>y, J = f(x)</code> where <code>y</code> is a vector output and <code>J</code> is the Jacobian.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L400-L405">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.xavier_init" href="#ADCME.xavier_init"><code>ADCME.xavier_init</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">xavier_init(size, dtype=Float64)</code></pre><p>Returns a matrix of size <code>size</code> and its values are from Xavier initialization. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L17-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.compile-Tuple{String}" href="#ADCME.compile-Tuple{String}"><code>ADCME.compile</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">compile(s::String; force::Bool=false)</code></pre><p>Compiles the library given by path <code>deps/s</code>. If <code>force</code> is false, <code>compile</code> first check whether  the binary product exists. If the binary product exists, return 2. Otherwise, <code>compile</code> tries to  compile the binary product, and returns 0 if successful; it return 1 otherwise. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L276-L282">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.test_gpu-Tuple{}" href="#ADCME.test_gpu-Tuple{}"><code>ADCME.test_gpu</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">test_gpu()</code></pre><p>Tests the GPU ultilities</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L867-L871">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.precompile" href="#Base.precompile"><code>Base.precompile</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">precompile(force::Bool=true)</code></pre><p>Compiles all the operators in <code>formulas.txt</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/extra.jl#L326-L330">source</a></section></article><h2 id="ODE-1"><a class="docs-heading-anchor" href="#ODE-1">ODE</a><a class="docs-heading-anchor-permalink" href="#ODE-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.ode45-Tuple" href="#ADCME.ode45-Tuple"><code>ADCME.ode45</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ode45(y::Union{PyObject, Float64, Array{Float64}}, T::Union{PyObject, Float64}, 
            NT::Union{PyObject,Int64}, f::Function, θ::Union{PyObject, Missing}=missing)</code></pre><p>Solves </p><div>\[\frac{dy}{dt} = f(y, t, \theta)\]</div><p>with six-stage, fifth-order, Runge-Kutta method.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ode.jl#L86-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.rk4-Tuple" href="#ADCME.rk4-Tuple"><code>ADCME.rk4</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">rk4(y::Union{PyObject, Float64, Array{Float64}}, T::Union{PyObject, Float64}, 
            NT::Union{PyObject,Int64}, f::Function, θ::Union{PyObject, Missing}=missing)</code></pre><p>Solves </p><div>\[\frac{dy}{dt} = f(y, t, \theta)\]</div><p>with Runge-Kutta (order 4) method. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ode.jl#L74-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.αscheme-Tuple{Union{SparseTensor, SparseArrays.SparseMatrixCSC},Union{SparseTensor, SparseArrays.SparseMatrixCSC},Union{SparseTensor, SparseArrays.SparseMatrixCSC},Union{Array{Float64,2}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject},Array{Float64,1}}" href="#ADCME.αscheme-Tuple{Union{SparseTensor, SparseArrays.SparseMatrixCSC},Union{SparseTensor, SparseArrays.SparseMatrixCSC},Union{SparseTensor, SparseArrays.SparseMatrixCSC},Union{Array{Float64,2}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject},Union{Array{Float64,1}, PyCall.PyObject},Array{Float64,1}}"><code>ADCME.αscheme</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">αscheme(M::Union{SparseTensor, SparseMatrixCSC}, 
    C::Union{SparseTensor, SparseMatrixCSC}, 
    K::Union{SparseTensor, SparseMatrixCSC}, 
    Force::Union{Array{Float64}, PyObject}, 
    d0::Union{Array{Float64, 1}, PyObject}, 
    v0::Union{Array{Float64, 1}, PyObject}, 
    a0::Union{Array{Float64, 1}, PyObject}, 
    Δt::Array{Float64}; 
    solve::Union{Missing, Function} = missing,
    extsolve::Union{Missing, Function} = missing, 
    ρ::Float64 = 1.0)</code></pre><p>Generalized α-scheme.  <span>$M u_{tt} + C u_{t} + K u = F$</span></p><p><code>Force</code> must be an array of size <code>n</code>×<code>p</code>, where <code>d0</code>, <code>v0</code>, and <code>a0</code> have a size <code>p</code> <code>Δt</code> is an array (variable time step). </p><p>The generalized α scheme solves the equation by the time stepping</p><div>\[\begin{aligned}
\bf d_{n+1} &amp;= \bf d_n + h\bf v_n + h^2 \left(\left(\frac{1}{2}-\beta_2 \right)\bf a_n + \beta_2 \bf a_{n+1}  \right)\\
\bf v_{n+1} &amp;= \bf v_n + h((1-\gamma_2)\bf a_n + \gamma_2 \bf a_{n+1})\\
\bf F(t_{n+1-\alpha_{f_2}}) &amp;= M \bf a _{n+1-\alpha_{m_2}} + C \bf v_{n+1-\alpha_{f_2}} + K \bf{d}_{n+1-\alpha_{f_2}}
\end{aligned}\]</div><p>where </p><div>\[\begin{aligned}
\bf d_{n+1-\alpha_{f_2}} &amp;= (1-\alpha_{f_2})\bf d_{n+1} + \alpha_{f_2} \bf d_n\\
\bf v_{n+1-\alpha_{f_2}} &amp;= (1-\alpha_{f_2}) \bf v_{n+1} + \alpha_{f_2} \bf v_n \\
\bf a_{n+1-\alpha_{m_2} } &amp;= (1-\alpha_{m_2}) \bf a_{n+1} + \alpha_{m_2} \bf a_n\\
t_{n+1-\alpha_{f_2}} &amp; = (1-\alpha_{f_2}) t_{n+1 + \alpha_{f_2}} + \alpha_{f_2}t_n
\end{aligned}\]</div><p>Here the parameters are computed using </p><div>\[\begin{aligned}
\gamma_2 &amp;= \frac{1}{2} - \alpha_{m_2} + \alpha_{f_2}\\
\beta_2 &amp;= \frac{1}{4} (1-\alpha_{m_2}+\alpha_{f_2})^2 \\
\alpha_{m_2} &amp;= \frac{2\rho_\infty-1}{\rho_\infty+1}\\
\alpha_{f_2} &amp;= \frac{\rho_\infty}{\rho_\infty+1}
\end{aligned}\]</div><p>∘ <code>solve</code>: users can provide a solver function, <code>solve(A, rhs)</code> for solving <code>Ax = rhs</code> ∘ <code>extsolve</code>: similar to <code>solve</code>, but the signature has the form </p><pre><code class="language-julia">extsolve(A, rhs, i)</code></pre><p>This provides the users with more control, e.g., (time-dependent) Dirichlet boundary conditions.  See <a href="https://kailaix.github.io/ADCME.jl/dev/alphascheme/">Generalized α Scheme</a> for details.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>In the case <span>$u$</span> has a nonzero essential boundary condition <span>$u_b$</span>, we let <span>$\tilde u=u-u_b$</span>, then  <span>$M \tilde u_{tt} + C \tilde u_t + K u = F - K u_b - C \dot u_b$</span></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ode.jl#L99-L157">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.αscheme_time-Tuple{Array{Float64,N} where N}" href="#ADCME.αscheme_time-Tuple{Array{Float64,N} where N}"><code>ADCME.αscheme_time</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">αscheme_time(Δt::Array{Float64}; ρ::Float64 = 1.0)</code></pre><p>Returns the integration time <span>$t_{i+1-\alpha_{f_2}}$</span> between <span>$[t_i, t_{i+1}]$</span> using the alpha scheme.  If <span>$\Delta t$</span> has length <span>$n$</span>, the output will also have length <span>$n$</span>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ode.jl#L228-L233">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.runge_kutta" href="#ADCME.runge_kutta"><code>ADCME.runge_kutta</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">runge_kutta(f::Function, T::Union{PyObject, Float64}, 
            NT::Union{PyObject,Int64}, y::Union{PyObject, Float64, Array{Float64}}, θ::Union{PyObject, Missing}=missing; method::String=&quot;rk4&quot;)</code></pre><p>Solves </p><div>\[\frac{dy}{dt} = f(y, t, \theta)\]</div><p>with Runge-Kutta method. </p><p>For example, the default solver, <code>RK4</code>, has the following numerical scheme per time step </p><div>\[\begin{aligned}
k_1 &amp;= \Delta t f(t_n, y_n, \theta)\\
k_2 &amp;= \Delta t f(t_n+\Delta t/2, y_n + k_1/2, \theta)\\
k_3 &amp;= \Delta t f(t_n+\Delta t/2, y_n + k_2/2, \theta)\\
k_4 &amp;= \Delta t f(t_n+\Delta t, y_n + k_3, \theta)\\
y_{n+1} &amp;= y_n + \frac{k_1}{6} +\frac{k_2}{3} +\frac{k_3}{3} +\frac{k_4}{6}
\end{aligned}\]</div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ode.jl#L23-L43">source</a></section></article><h2 id="Optimal-Transport-1"><a class="docs-heading-anchor" href="#Optimal-Transport-1">Optimal Transport</a><a class="docs-heading-anchor-permalink" href="#Optimal-Transport-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ADCME.dist" href="#ADCME.dist"><code>ADCME.dist</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">dist(x::Union{PyObject, Array{Float64}}, y::Union{PyObject, Array{Float64}}, order::Union{Int64, PyObject}=2)</code></pre><p>Computes the distance function with norm <code>order</code>. <code>dist</code> returns a <span>$n\times m$</span> matrix, where <span>$x\in \mathbb{R}^{n\times d}$</span> and <span>$y\in \mathbb{R}^{m\times d}$</span>, and the return <span>$M\in \mathbb{R}^{n\times m}$</span></p><div>\[M_{ij} = ||x_i - y_j||_{o}\]</div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ot.jl#L66-L74">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.dtw" href="#ADCME.dtw"><code>ADCME.dtw</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">dtw(s::Union{PyObject, Array{Float64}}, t::Union{PyObject, Array{Float64}}, 
    use_fast::Bool = false)</code></pre><p>Computes the dynamic time wrapping (DTW) distance between two time series <code>s</code> and <code>t</code>.  Returns the distance and path. <code>use_fast</code> specifies whether fast algorithm is used. Note  fast algorithm may not be accurate.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ot.jl#L84-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.empirical_sinkhorn-Tuple{Union{PyCall.PyObject, Array{Float64,N} where N},Union{PyCall.PyObject, Array{Float64,N} where N}}" href="#ADCME.empirical_sinkhorn-Tuple{Union{PyCall.PyObject, Array{Float64,N} where N},Union{PyCall.PyObject, Array{Float64,N} where N}}"><code>ADCME.empirical_sinkhorn</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">empirical_sinkhorn(x::Union{PyObject, Array{Float64}}, y::Union{PyObject, Array{Float64}}, dist::Function;
reg::Union{PyObject,Float64} = 1.0, iter::Int64 = 1000, tol::Float64 = 1e-9, method::String=&quot;sinkhorn&quot;)</code></pre><p>Computes the empirical Wasserstein distance with sinkhorn algorithm.  The implementation are adapted from https://github.com/rflamary/POT.  </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ot.jl#L47-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ADCME.sinkhorn-Tuple{Union{PyCall.PyObject, Array{Float64,N} where N},Union{PyCall.PyObject, Array{Float64,N} where N},Union{PyCall.PyObject, Array{Float64,N} where N}}" href="#ADCME.sinkhorn-Tuple{Union{PyCall.PyObject, Array{Float64,N} where N},Union{PyCall.PyObject, Array{Float64,N} where N},Union{PyCall.PyObject, Array{Float64,N} where N}}"><code>ADCME.sinkhorn</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">sinkhorn(a::Union{PyObject, Array{Float64}}, b::Union{PyObject, Array{Float64}}, M::Union{PyObject, Array{Float64}};
reg::Float64 = 1.0, iter::Int64 = 1000, tol::Float64 = 1e-9, method::String=&quot;sinkhorn&quot;)</code></pre><p>Computes the optimal transport with Sinkhorn algorithm.  The implementation are adapted from https://github.com/rflamary/POT.  </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kailaix/ADCME.jl/blob/22f31e31aae852a4f7385b04be4686f87fe8a272/src/ot.jl#L4-L10">source</a></section></article><h2 id="Misc-1"><a class="docs-heading-anchor" href="#Misc-1">Misc</a><a class="docs-heading-anchor-permalink" href="#Misc-1" title="Permalink"></a></h2></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../apps_nnfem/">« Symmetric Positive Definite Neural Networks (SPD-NN)</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 2 June 2020 17:27">Tuesday 2 June 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
